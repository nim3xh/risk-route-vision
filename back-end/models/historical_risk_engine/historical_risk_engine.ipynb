{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgzvr7iFe6HK",
        "outputId": "f8bc4f55-1748-48c7-c606-75035d9ea9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cause Classifier — Test Metrics ===\n",
            "Accuracy        : 0.9412\n",
            "Precision (macro): 0.6548 | Recall (macro): 0.7308 | F1 (macro): 0.6839\n",
            "Precision (weighted): 0.9356 | Recall (weighted): 0.9412 | F1 (weighted): 0.9347\n",
            "[Info] Classification metrics saved to: content/outputs/classification_metrics.json\n",
            "\n",
            "=== Summary ===\n",
            "Rows (raw): 315\n",
            "Rows (final): 315 | Columns (final): 23\n",
            "Cause classifier Macro-F1: 0.684\n",
            "Segment rate RMSE: 0.109\n",
            "Saved:\n",
            " - content/outputs/final_dataset.csv\n",
            " - content/outputs/final_dataset_min.csv\n",
            " - content/outputs/risk_tiles.csv\n",
            " - content/models/cause_classifier.joblib\n",
            " - content/models/segment_gbr.joblib\n",
            " - Plots in content/outputs\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import os, json, math, warnings, string\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, f1_score, mean_squared_error,\n",
        "    accuracy_score, precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "import joblib\n",
        "from textwrap import wrap\n",
        "\n",
        "# CONFIG\n",
        "RAW_EXCEL_PATH = \"dataset final.xlsx\"\n",
        "OUT_DIR = \"content/outputs\"\n",
        "MODELS_DIR = \"content/models\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "@dataclass\n",
        "class PipelineOutputs:\n",
        "    df_final: pd.DataFrame\n",
        "    df_final_min: pd.DataFrame\n",
        "    risk_tiles: pd.DataFrame\n",
        "    cause_report: Dict[str, Any]\n",
        "    cause_macro_f1: float\n",
        "    cause_conf_mat: np.ndarray\n",
        "    cause_classes: List[str]\n",
        "    gbr_rmse: float\n",
        "    cause_accuracy: float\n",
        "    cause_precision_macro: float\n",
        "    cause_recall_macro: float\n",
        "    cause_f1_macro: float\n",
        "    cause_precision_weighted: float\n",
        "    cause_recall_weighted: float\n",
        "    cause_f1_weighted: float\n",
        "\n",
        "# UTILITIES\n",
        "def _titlecase(s: str) -> str:\n",
        "    if s is None:\n",
        "        return s\n",
        "    s = str(s).strip()\n",
        "    if not s:\n",
        "        return s\n",
        "    return string.capwords(s)\n",
        "\n",
        "def _read_excel_first_sheet(path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    return pd.read_excel(path, sheet_name=0)\n",
        "\n",
        "def _drop_slope_elevation(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    to_drop = [c for c in df.columns if (\"slope\" in c.lower()) or (\"elev\" in c.lower())]\n",
        "    return df.drop(columns=to_drop, errors=\"ignore\")\n",
        "\n",
        "def _build_timestamp(df: pd.DataFrame) -> pd.Series:\n",
        "    ts = None\n",
        "    for cand in [\"Datetime\",\"DateTime\",\"Timestamp\",\"DATE_TIME\",\"date_time\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            break\n",
        "    if ts is None:\n",
        "        if \"Date\" in df.columns and \"Time\" in df.columns:\n",
        "            ts = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str), errors=\"coerce\")\n",
        "        elif \"Date\" in df.columns:\n",
        "            ts = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "        else:\n",
        "            ts = pd.Series([pd.NaT]*len(df), index=df.index)\n",
        "    return ts\n",
        "\n",
        "def _clean_reason(series: pd.Series) -> pd.Series:\n",
        "    s = series.astype(str)\n",
        "    missing_like = s.str.strip().str.lower().isin({\"nan\",\"na\",\"n/a\",\"none\",\"-\",\"\"})\n",
        "    s = s.mask(missing_like, np.nan)\n",
        "    s = s.where(s.isna(), s.str.strip().str.title())\n",
        "    return s\n",
        "\n",
        "def feature_engineer(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_raw.copy()\n",
        "    df = _drop_slope_elevation(df)\n",
        "\n",
        "    # Categories\n",
        "    for col in [\"Vehicle\",\"Place\",\"Position\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).map(_titlecase)\n",
        "\n",
        "    # Reason\n",
        "    if \"Reason\" in df.columns:\n",
        "        df[\"Reason\"] = _clean_reason(df[\"Reason\"])\n",
        "\n",
        "    # Timestamp\n",
        "    df[\"ts\"] = _build_timestamp(df)\n",
        "\n",
        "    # Derived features\n",
        "    if \"ts\" in df.columns:\n",
        "        df[\"hour\"] = pd.to_datetime(df[\"ts\"]).dt.hour\n",
        "        df[\"dow\"] = pd.to_datetime(df[\"ts\"]).dt.dayofweek\n",
        "        df[\"is_weekend\"] = df[\"dow\"].isin([5,6]).astype(int)\n",
        "\n",
        "    # Wet flag from precip\n",
        "    precip_col = next((c for c in df.columns if \"precip\" in c.lower()), None)\n",
        "    if precip_col:\n",
        "        df[\"is_wet\"] = (pd.to_numeric(df[precip_col], errors=\"coerce\").fillna(0) > 0.1).astype(int)\n",
        "    else:\n",
        "        df[\"is_wet\"] = 0\n",
        "\n",
        "    # Lat/Lon bins + segment_id\n",
        "    if \"Latitude\" in df.columns and \"Longitude\" in df.columns:\n",
        "        df[\"lat_bin\"] = pd.to_numeric(df[\"Latitude\"], errors=\"coerce\").round(3)\n",
        "        df[\"lon_bin\"] = pd.to_numeric(df[\"Longitude\"], errors=\"coerce\").round(3)\n",
        "        df[\"segment_id\"] = df[\"lat_bin\"].astype(str) + \"_\" + df[\"lon_bin\"].astype(str)\n",
        "    else:\n",
        "        df[\"lat_bin\"] = np.nan\n",
        "        df[\"lon_bin\"] = np.nan\n",
        "        df[\"segment_id\"] = \"NA\"\n",
        "\n",
        "    # Speed-related indicator\n",
        "    df[\"is_speed_reason\"] = df.get(\"Reason\", \"\").astype(str).str.contains(\"Excessive Speed\", case=False, na=False).astype(int)\n",
        "\n",
        "    # Ensure numeric types\n",
        "    for c in [\"Temperature (C)\",\"Humidity (%)\",\"Precipitation (mm)\",\"Wind Speed (km/h)\",\"Latitude\",\"Longitude\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_spi(df: pd.DataFrame, alpha: float = 20.0) -> pd.DataFrame:\n",
        "    \"\"\"Speed Propensity Index (SPI) per (lat_bin, lon_bin, hour, is_wet, Vehicle), smoothed with alpha.\"\"\"\n",
        "    global_spi = float(df[\"is_speed_reason\"].mean()) if \"is_speed_reason\" in df.columns else 0.0\n",
        "    keys = [\"lat_bin\",\"lon_bin\",\"hour\",\"is_wet\",\"Vehicle\"]\n",
        "    for k in keys:\n",
        "        if k not in df.columns:\n",
        "            df[k] = np.nan\n",
        "    grp = (df.groupby(keys, dropna=False)\n",
        "             .agg(spi=(\"is_speed_reason\",\"mean\"), n=(\"is_speed_reason\",\"size\"))\n",
        "             .reset_index())\n",
        "    grp[\"SPI_smoothed\"] = (grp[\"n\"]*grp[\"spi\"] + alpha*global_spi) / (grp[\"n\"] + alpha)\n",
        "    df_spi = df.merge(grp[keys + [\"SPI_smoothed\"]], on=keys, how=\"left\")\n",
        "    df_spi[\"SPI_smoothed\"] = df_spi[\"SPI_smoothed\"].fillna(global_spi)\n",
        "    return df_spi\n",
        "\n",
        "def make_final_datasets(df_spi: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    full = df_spi.copy()\n",
        "    keep_cols = [\n",
        "        \"ts\",\"Date\",\"Time\",\"Latitude\",\"Longitude\",\"lat_bin\",\"lon_bin\",\"segment_id\",\n",
        "        \"Vehicle\",\"Place\",\"Position\",\n",
        "        \"Reason\",\"Description\",\n",
        "        \"Temperature (C)\",\"Humidity (%)\",\"Precipitation (mm)\",\"Wind Speed (km/h)\",\n",
        "        \"hour\",\"dow\",\"is_weekend\",\"is_wet\",\"is_speed_reason\",\"SPI_smoothed\"\n",
        "    ]\n",
        "    min_cols = [c for c in keep_cols if c in full.columns]\n",
        "    minimal = full[min_cols].copy()\n",
        "    return full, minimal\n",
        "\n",
        "def build_risk_tiles(df_spi: pd.DataFrame) -> pd.DataFrame:\n",
        "    keys = [\"segment_id\",\"lat_bin\",\"lon_bin\",\"hour\",\"dow\",\"is_wet\",\"Vehicle\"]\n",
        "    for k in keys:\n",
        "        if k not in df_spi.columns:\n",
        "            df_spi[k] = np.nan\n",
        "    agg = (df_spi.groupby(keys, dropna=False)\n",
        "               .agg(incident_count=(\"Reason\",\"size\"),\n",
        "                    speed_reason_rate=(\"is_speed_reason\",\"mean\"),\n",
        "                    n=(\"is_speed_reason\",\"size\"),\n",
        "                    SPI_tile=(\"SPI_smoothed\",\"mean\"))\n",
        "               .reset_index())\n",
        "    return agg\n",
        "\n",
        "def plot_bar_counts(series: pd.Series, title: str, out_path: str, top_n: int = 15):\n",
        "    counts = series.value_counts().head(top_n)\n",
        "    plt.figure()\n",
        "    counts.plot(kind=\"bar\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_hourly_counts(df: pd.DataFrame, out_path: str):\n",
        "    plt.figure()\n",
        "    counts = df[\"hour\"].value_counts().sort_index()\n",
        "    counts.plot(kind=\"bar\")\n",
        "    plt.title(\"Incidents by Hour of Day\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_geo_scatter(df: pd.DataFrame, out_path: str):\n",
        "    if \"Longitude\" not in df.columns or \"Latitude\" not in df.columns:\n",
        "        return\n",
        "    plt.figure()\n",
        "    plt.scatter(df[\"Longitude\"], df[\"Latitude\"], s=10)\n",
        "    plt.title(\"Incident Locations (Lon/Lat)\")\n",
        "    plt.xlabel(\"Longitude\")\n",
        "    plt.ylabel(\"Latitude\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix_pretty(cm, classes, title=\"Cause Classifier - Confusion Matrix\",\n",
        "                                 normalize=False, out_path=None, dpi=220):\n",
        "    \"\"\"\n",
        "    cm: numpy.ndarray (n_classes x n_classes)\n",
        "    classes: list[str] in the same order used to build 'cm'\n",
        "    normalize: if True -> each row is shown as percentages\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    cm = np.array(cm, dtype=float)\n",
        "\n",
        "    # Row-normalize for percentage view\n",
        "    if normalize:\n",
        "        rs = cm.sum(axis=1, keepdims=True)\n",
        "        rs[rs == 0] = 1.0\n",
        "        cm_disp = cm / rs\n",
        "    else:\n",
        "        cm_disp = cm.copy()\n",
        "\n",
        "    n = cm.shape[0]\n",
        "    labels_wrapped = [\"\\n\".join(wrap(c, 18)) for c in classes]\n",
        "\n",
        "    plt.figure(figsize=(9, 7), dpi=dpi)\n",
        "    im = plt.imshow(cm_disp, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    plt.title(title, pad=10)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.xticks(range(n), labels_wrapped, rotation=35, ha=\"right\")\n",
        "    plt.yticks(range(n), labels_wrapped)\n",
        "\n",
        "    # grid\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(-.5, n, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, n, 1), minor=True)\n",
        "    ax.grid(which='minor', linestyle='-', linewidth=0.5, alpha=0.4)\n",
        "    ax.tick_params(which='minor', bottom=False, left=False)\n",
        "\n",
        "    row_totals = cm.sum(axis=1, keepdims=True)\n",
        "    row_totals[row_totals == 0] = 1.0\n",
        "    thresh = cm_disp.max() / 2.0 if cm_disp.max() > 0 else 0.5\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            cnt = int(round(cm[i, j]))\n",
        "            pct = (cm[i, j] / row_totals[i, 0]) * 100.0\n",
        "            text = f\"{cnt}\\n{pct:.0f}%\"\n",
        "            color = \"white\" if cm_disp[i, j] > thresh else \"black\"\n",
        "            plt.text(j, i, text, ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
        "\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.tight_layout()\n",
        "    if out_path:\n",
        "        plt.savefig(out_path, bbox_inches=\"tight\", dpi=dpi)\n",
        "    plt.close()\n",
        "\n",
        "def train_cause_classifier(df_final_min: pd.DataFrame):\n",
        "    work = df_final_min.dropna(subset=[\"Reason\",\"ts\"]).copy()\n",
        "    work = work.sort_values(\"ts\")\n",
        "    if \"Description\" in work.columns:\n",
        "        work[\"Description\"] = work[\"Description\"].fillna(\"\").astype(str)\n",
        "\n",
        "    n = len(work)\n",
        "    if n < 10:\n",
        "        raise RuntimeError(\"Not enough records with timestamp + reason to train the classifier.\")\n",
        "    split_idx = max(int(0.8 * n), 1)\n",
        "    train_df = work.iloc[:split_idx]\n",
        "    test_df  = work.iloc[split_idx:]\n",
        "\n",
        "    numeric_cols = [c for c in [\"Temperature (C)\",\"Humidity (%)\",\"Precipitation (mm)\",\"Wind Speed (km/h)\",\n",
        "                                \"hour\",\"dow\",\"is_weekend\",\"is_wet\",\"Latitude\",\"Longitude\",\"SPI_smoothed\"]\n",
        "                    if c in work.columns]\n",
        "    cat_cols = [c for c in [\"Vehicle\",\"Place\",\"Position\"] if c in work.columns]\n",
        "    text_col = \"Description\" if \"Description\" in work.columns else None\n",
        "\n",
        "    num_pipe = Pipeline([\n",
        "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scale\", StandardScaler())\n",
        "    ])\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    transformers = [(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, cat_cols)]\n",
        "    if text_col:\n",
        "        transformers.append((\"txt\", TfidfVectorizer(min_df=3, ngram_range=(1,2)), text_col))\n",
        "\n",
        "    pre = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0.3)\n",
        "    clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"liblinear\")\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "\n",
        "    X_train = train_df[numeric_cols + cat_cols + ([text_col] if text_col else [])]\n",
        "    y_train = train_df[\"Reason\"]\n",
        "    X_test  = test_df[numeric_cols + cat_cols + ([text_col] if text_col else [])]\n",
        "    y_test  = test_df[\"Reason\"]\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    # Full classification report (dict) + macro F1 (legacy kept)\n",
        "    report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "    macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "    classes = list(pipe.classes_)\n",
        "    conf_mat = confusion_matrix(y_test, y_pred, labels=classes)\n",
        "\n",
        "    # Explicit metrics\n",
        "    accuracy = float(accuracy_score(y_test, y_pred))\n",
        "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    prec_w, rec_w, f1_w, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "\n",
        "    # Save matrices\n",
        "    plot_confusion_matrix_pretty(\n",
        "        conf_mat, classes,\n",
        "        normalize=False,\n",
        "        out_path=os.path.join(OUT_DIR, \"confusion_matrix_counts.png\")\n",
        "    )\n",
        "    plot_confusion_matrix_pretty(\n",
        "        conf_mat, classes,\n",
        "        normalize=True,\n",
        "        out_path=os.path.join(OUT_DIR, \"confusion_matrix_normalized.png\")\n",
        "    )\n",
        "\n",
        "    return (pipe, report_dict, macro_f1, conf_mat, classes,\n",
        "            accuracy, float(prec_macro), float(rec_macro), float(f1_macro),\n",
        "            float(prec_w), float(rec_w), float(f1_w))\n",
        "\n",
        "def train_segment_rate_model(risk_tiles: pd.DataFrame):\n",
        "    data = risk_tiles.copy()\n",
        "    feat = [\"hour\",\"dow\",\"is_wet\",\"Vehicle\"]\n",
        "    for f in [\"hour\",\"dow\",\"is_wet\"]:\n",
        "        if f not in data.columns:\n",
        "            data[f] = 0\n",
        "    X = pd.get_dummies(data[feat], columns=[\"Vehicle\"], dummy_na=True)\n",
        "    y = data[\"incident_count\"].astype(float)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    gbr = HistGradientBoostingRegressor(max_depth=3, random_state=42)\n",
        "    gbr.fit(X_train, y_train)\n",
        "    pred = gbr.predict(X_test)\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_test, pred)))\n",
        "    return gbr, rmse\n",
        "\n",
        "def save_metrics_json(report_dict: Dict[str, Any], macro_f1: float, gbr_rmse: float, out_path: str,\n",
        "                      extra_cls: Dict[str, float]):\n",
        "    payload = {\n",
        "        \"cause_classifier\": {\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"per_class\": {k:v for k,v in report_dict.items() if k not in {\"accuracy\",\"macro avg\",\"weighted avg\"}},\n",
        "            \"macro_avg\": report_dict.get(\"macro avg\", {}),\n",
        "            \"weighted_avg\": report_dict.get(\"weighted avg\", {}),\n",
        "            \"accuracy\": report_dict.get(\"accuracy\", None),\n",
        "            \"accuracy_explicit\": extra_cls.get(\"accuracy\"),\n",
        "            \"precision_macro\": extra_cls.get(\"precision_macro\"),\n",
        "            \"recall_macro\": extra_cls.get(\"recall_macro\"),\n",
        "            \"f1_macro\": extra_cls.get(\"f1_macro\"),\n",
        "            \"precision_weighted\": extra_cls.get(\"precision_weighted\"),\n",
        "            \"recall_weighted\": extra_cls.get(\"recall_weighted\"),\n",
        "            \"f1_weighted\": extra_cls.get(\"f1_weighted\"),\n",
        "        },\n",
        "        \"segment_rate_model\": {\n",
        "            \"rmse\": gbr_rmse\n",
        "        }\n",
        "    }\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2)\n",
        "\n",
        "def main() -> PipelineOutputs:\n",
        "    df_raw = _read_excel_first_sheet(RAW_EXCEL_PATH)\n",
        "    df_feat = feature_engineer(df_raw)\n",
        "\n",
        "    # SPI\n",
        "    df_spi = build_spi(df_feat, alpha=20.0)\n",
        "\n",
        "    df_full, df_min = make_final_datasets(df_spi)\n",
        "    risk_tiles = build_risk_tiles(df_spi)\n",
        "\n",
        "    final_full_path = os.path.join(OUT_DIR, \"final_dataset.csv\")\n",
        "    final_min_path  = os.path.join(OUT_DIR, \"final_dataset_min.csv\")\n",
        "    risk_tiles_path = os.path.join(OUT_DIR, \"risk_tiles.csv\")\n",
        "    df_full.to_csv(final_full_path, index=False)\n",
        "    df_min.to_csv(final_min_path, index=False)\n",
        "    risk_tiles.to_csv(risk_tiles_path, index=False)\n",
        "\n",
        "    if \"Reason\" in df_min.columns:\n",
        "        plot_bar_counts(df_min[\"Reason\"].dropna(), \"Top Reasons\", os.path.join(OUT_DIR, \"top_reasons.png\"))\n",
        "    if \"Vehicle\" in df_min.columns:\n",
        "        plot_bar_counts(df_min[\"Vehicle\"].dropna(), \"Top Vehicles\", os.path.join(OUT_DIR, \"top_vehicles.png\"))\n",
        "    if \"hour\" in df_min.columns:\n",
        "        plot_hourly_counts(df_min, os.path.join(OUT_DIR, \"incidents_by_hour.png\"))\n",
        "    plot_geo_scatter(df_min, os.path.join(OUT_DIR, \"geo_scatter.png\"))\n",
        "\n",
        "    # Train cause-of-incident classifier\n",
        "    (pipe, report_dict, macro_f1, conf_mat, classes,\n",
        "     acc, p_macro, r_macro, f_macro, p_w, r_w, f_w) = train_cause_classifier(df_min)\n",
        "    joblib.dump(pipe, os.path.join(MODELS_DIR, \"cause_classifier.joblib\"))\n",
        "\n",
        "    print(\"\\n=== Cause Classifier — Test Metrics ===\")\n",
        "    print(f\"Accuracy        : {acc:.4f}\")\n",
        "    print(f\"Precision (macro): {p_macro:.4f} | Recall (macro): {r_macro:.4f} | F1 (macro): {f_macro:.4f}\")\n",
        "    print(f\"Precision (weighted): {p_w:.4f} | Recall (weighted): {r_w:.4f} | F1 (weighted): {f_w:.4f}\")\n",
        "\n",
        "    cls_metrics_path = os.path.join(OUT_DIR, \"classification_metrics.json\")\n",
        "    with open(cls_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": p_macro,\n",
        "            \"recall_macro\": r_macro,\n",
        "            \"f1_macro\": f_macro,\n",
        "            \"precision_weighted\": p_w,\n",
        "            \"recall_weighted\": r_w,\n",
        "            \"f1_weighted\": f_w,\n",
        "            \"classes\": classes\n",
        "        }, f, indent=2)\n",
        "    print(f\"[Info] Classification metrics saved to: {cls_metrics_path}\")\n",
        "\n",
        "    # Train segment risk model\n",
        "    gbr, rmse = train_segment_rate_model(risk_tiles)\n",
        "    joblib.dump(gbr, os.path.join(MODELS_DIR, \"segment_gbr.joblib\"))\n",
        "\n",
        "    save_metrics_json(\n",
        "        report_dict, macro_f1, rmse, os.path.join(OUT_DIR, \"metrics.json\"),\n",
        "        extra_cls={\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": p_macro,\n",
        "            \"recall_macro\": r_macro,\n",
        "            \"f1_macro\": f_macro,\n",
        "            \"precision_weighted\": p_w,\n",
        "            \"recall_weighted\": r_w,\n",
        "            \"f1_weighted\": f_w\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Summary ===\")\n",
        "    print(f\"Rows (raw): {len(df_raw)}\")\n",
        "    print(f\"Rows (final): {len(df_min)} | Columns (final): {df_min.shape[1]}\")\n",
        "    print(f\"Cause classifier Macro-F1: {macro_f1:.3f}\")\n",
        "    print(f\"Segment rate RMSE: {rmse:.3f}\")\n",
        "    print(\"Saved:\")\n",
        "    print(f\" - {final_full_path}\")\n",
        "    print(f\" - {final_min_path}\")\n",
        "    print(f\" - {risk_tiles_path}\")\n",
        "    print(f\" - {os.path.join(MODELS_DIR, 'cause_classifier.joblib')}\")\n",
        "    print(f\" - {os.path.join(MODELS_DIR, 'segment_gbr.joblib')}\")\n",
        "    print(f\" - Plots in {OUT_DIR}\")\n",
        "    return PipelineOutputs(\n",
        "        df_final=df_full,\n",
        "        df_final_min=df_min,\n",
        "        risk_tiles=risk_tiles,\n",
        "        cause_report=report_dict,\n",
        "        cause_macro_f1=macro_f1,\n",
        "        cause_conf_mat=conf_mat,\n",
        "        cause_classes=classes,\n",
        "        gbr_rmse=rmse,\n",
        "        cause_accuracy=acc,\n",
        "        cause_precision_macro=p_macro,\n",
        "        cause_recall_macro=r_macro,\n",
        "        cause_f1_macro=f_macro,\n",
        "        cause_precision_weighted=p_w,\n",
        "        cause_recall_weighted=r_w,\n",
        "        cause_f1_weighted=f_w\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, Optional\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Config\n",
        "CAUSE_MODEL_PATH = \"content/models/cause_classifier.joblib\"\n",
        "RATE_MODEL_PATH  = \"content/models/segment_gbr.joblib\"\n",
        "RISK_TILES_CSV   = \"content/outputs/risk_tiles.csv\"\n",
        "\n",
        "def _title(s: Optional[str]) -> str:\n",
        "    return (s or \"\").strip().title()\n",
        "\n",
        "def _sigmoid(x: float) -> float:\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "@dataclass\n",
        "class RiskInputs:\n",
        "    lat: float\n",
        "    lon: float\n",
        "    dt: datetime\n",
        "    is_wet: int\n",
        "    vehicle: str\n",
        "    place: Optional[str] = None\n",
        "    position: Optional[str] = None\n",
        "    temperature_c: Optional[float] = None\n",
        "    humidity_pct: Optional[float] = None\n",
        "    precip_mm: Optional[float] = None\n",
        "    wind_kmh: Optional[float] = None\n",
        "    description: Optional[str] = \"\"\n",
        "\n",
        "class RiskModel:\n",
        "    def __init__(self,\n",
        "                 cause_model_path: str = CAUSE_MODEL_PATH,\n",
        "                 rate_model_path: str = RATE_MODEL_PATH,\n",
        "                 risk_tiles_csv: str = RISK_TILES_CSV):\n",
        "        # Load models\n",
        "        self.cause_pipe = joblib.load(cause_model_path)\n",
        "        self.rate_model = joblib.load(rate_model_path)    # Histogram based Gradient Boosting Regressor\n",
        "        # Load risk tiles for SPI lookup & calibration\n",
        "        self.tiles = pd.read_csv(risk_tiles_csv)\n",
        "        # Precompute a few calibrators\n",
        "        self.global_spi = float(self.tiles[\"SPI_tile\"].mean()) if \"SPI_tile\" in self.tiles else 0.1\n",
        "        # scale for rate normalization\n",
        "        self.rate_scale = float(np.quantile(self.tiles[\"incident_count\"], 0.95)) if \"incident_count\" in self.tiles else 5.0\n",
        "        if self.rate_scale <= 0:\n",
        "            self.rate_scale = 1.0\n",
        "\n",
        "    # SPI lookup\n",
        "    def _spi_lookup(self, lat: float, lon: float, hour: int, dow: int, is_wet: int, vehicle: str) -> float:\n",
        "        v = _title(vehicle)\n",
        "        latb, lonb = round(float(lat), 3), round(float(lon), 3)\n",
        "        t = self.tiles\n",
        "\n",
        "        # exact match (including day of week)\n",
        "        m = (t[\"lat_bin\"].round(3).eq(latb) &\n",
        "             t[\"lon_bin\"].round(3).eq(lonb) &\n",
        "             t[\"hour\"].eq(hour) &\n",
        "             t[\"dow\"].eq(dow) &\n",
        "             t[\"is_wet\"].eq(is_wet) &\n",
        "             t[\"Vehicle\"].astype(str).str.title().eq(v))\n",
        "        cand = t.loc[m, \"SPI_tile\"]\n",
        "        if not cand.empty:\n",
        "            return float(cand.mean())\n",
        "\n",
        "        # ignore day of week\n",
        "        m = (t[\"lat_bin\"].round(3).eq(latb) &\n",
        "             t[\"lon_bin\"].round(3).eq(lonb) &\n",
        "             t[\"hour\"].eq(hour) &\n",
        "             t[\"is_wet\"].eq(is_wet) &\n",
        "             t[\"Vehicle\"].astype(str).str.title().eq(v))\n",
        "        cand = t.loc[m, \"SPI_tile\"]\n",
        "        if not cand.empty:\n",
        "            return float(cand.mean())\n",
        "\n",
        "        # ignore hour (keep location/wet/vehicle)\n",
        "        m = (t[\"lat_bin\"].round(3).eq(latb) &\n",
        "             t[\"lon_bin\"].round(3).eq(lonb) &\n",
        "             t[\"is_wet\"].eq(is_wet) &\n",
        "             t[\"Vehicle\"].astype(str).str.title().eq(v))\n",
        "        cand = t.loc[m, \"SPI_tile\"]\n",
        "        if not cand.empty:\n",
        "            return float(cand.mean())\n",
        "\n",
        "        # fallback to vehicle + wet anywhere\n",
        "        m = (t[\"is_wet\"].eq(is_wet) &\n",
        "             t[\"Vehicle\"].astype(str).str.title().eq(v))\n",
        "        cand = t.loc[m, \"SPI_tile\"]\n",
        "        if not cand.empty:\n",
        "            return float(cand.mean())\n",
        "\n",
        "        # global average\n",
        "        return self.global_spi\n",
        "\n",
        "    def _build_classifier_row(self, x: RiskInputs) -> pd.DataFrame:\n",
        "        hour = int(x.dt.hour)\n",
        "        dow  = int(x.dt.weekday())\n",
        "        is_weekend = 1 if dow in (5, 6) else 0\n",
        "        latb, lonb = round(float(x.lat), 3), round(float(x.lon), 3)\n",
        "        spi = self._spi_lookup(x.lat, x.lon, hour, dow, x.is_wet, x.vehicle)\n",
        "\n",
        "        row = {\n",
        "            \"Temperature (C)\": x.temperature_c,\n",
        "            \"Humidity (%)\": x.humidity_pct,\n",
        "            \"Precipitation (mm)\": x.precip_mm,\n",
        "            \"Wind Speed (km/h)\": x.wind_kmh,\n",
        "            \"Latitude\": x.lat,\n",
        "            \"Longitude\": x.lon,\n",
        "            \"hour\": hour,\n",
        "            \"dow\": dow,\n",
        "            \"is_weekend\": is_weekend,\n",
        "            \"is_wet\": int(x.is_wet),\n",
        "            \"SPI_smoothed\": spi,\n",
        "            \"Vehicle\": _title(x.vehicle),\n",
        "            \"Place\": _title(x.place),\n",
        "            \"Position\": _title(x.position),\n",
        "            \"Description\": (x.description or \"\")\n",
        "        }\n",
        "        return pd.DataFrame([row])\n",
        "\n",
        "    def predict_cause(self, x: RiskInputs) -> Dict[str, Any]:\n",
        "        row = self._build_classifier_row(x)\n",
        "        # Predict probability with the fitted pipeline\n",
        "        proba = self.cause_pipe.predict_proba(row)[0]\n",
        "        classes = list(self.cause_pipe.classes_)\n",
        "        idx = int(np.argmax(proba))\n",
        "        return {\n",
        "            \"top_cause\": classes[idx],\n",
        "            \"p_top_cause\": float(proba[idx]),\n",
        "            \"probs\": {c: float(p) for c, p in zip(classes, proba)}\n",
        "        }\n",
        "\n",
        "    def predict_segment_rate(self, x: RiskInputs) -> float:\n",
        "        # Rate model features: hour, DoW, is_wet, Vehicle\n",
        "        base = pd.DataFrame([{\n",
        "            \"hour\": int(x.dt.hour),\n",
        "            \"dow\": int(x.dt.weekday()),\n",
        "            \"is_wet\": int(x.is_wet),\n",
        "            \"Vehicle\": _title(x.vehicle)\n",
        "        }])\n",
        "        X = pd.get_dummies(base, columns=[\"Vehicle\"], dummy_na=True)\n",
        "        # Align columns to model\n",
        "        needed = list(self.rate_model.feature_names_in_)\n",
        "        for col in needed:\n",
        "            if col not in X.columns:\n",
        "                X[col] = 0\n",
        "        X = X[needed]\n",
        "        rate = float(max(0.0, self.rate_model.predict(X)[0]))\n",
        "        return rate\n",
        "\n",
        "    def score_risk(self, x: RiskInputs) -> Dict[str, Any]:\n",
        "        c = self.predict_cause(x)\n",
        "        r = self.predict_segment_rate(x)\n",
        "\n",
        "        # Normalize and combine\n",
        "        cause_component = _sigmoid(5.0 * (c[\"p_top_cause\"] - 0.5))\n",
        "        rate_component  = min(1.0, r / max(1e-6, self.rate_scale))\n",
        "\n",
        "        # Light context weights + vehicle/weather multipliers\n",
        "        base = 0.6 * cause_component + 0.4 * rate_component\n",
        "        S_vehicle = {\"Motor Cycle\": 1.2, \"Three Wheeler\": 1.1}.get(_title(x.vehicle), 1.0)\n",
        "        W_weather = 1.25 if x.is_wet else 1.0\n",
        "\n",
        "        risk = 100.0 * base * S_vehicle * W_weather\n",
        "        return {\n",
        "            \"risk_0_100\": float(min(100.0, max(0.0, round(risk, 1)))),\n",
        "            \"top_cause\": c[\"top_cause\"],\n",
        "            \"p_top_cause\": round(c[\"p_top_cause\"], 3),\n",
        "            \"rate_pred\": round(r, 3),\n",
        "            \"components\": {\n",
        "                \"cause_component\": round(cause_component, 3),\n",
        "                \"rate_component\": round(rate_component, 3),\n",
        "                \"S_vehicle\": S_vehicle,\n",
        "                \"W_weather\": W_weather\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Example\n",
        "if __name__ == \"__main__\":\n",
        "    from datetime import datetime\n",
        "    model = RiskModel()\n",
        "    ctx = RiskInputs(\n",
        "        lat=7.0192, lon=80.4943,\n",
        "        dt=datetime.now(),\n",
        "        is_wet=1,\n",
        "        vehicle=\"Bus\",\n",
        "        place=\"Ginigathena\",\n",
        "        position=\"Bend\",\n",
        "        temperature_c=24.0,\n",
        "        humidity_pct=92.0,\n",
        "        precip_mm=0.6,\n",
        "        wind_kmh=12.0,\n",
        "        description=\"\"\n",
        "    )\n",
        "    print(\"Cause:\", model.predict_cause(ctx))\n",
        "    print(\"Rate :\", model.predict_segment_rate(ctx))\n",
        "    print(\"Risk :\", model.score_risk(ctx))\n"
      ],
      "metadata": {
        "id": "VbmoC9DYinat",
        "outputId": "3a4706d4-5de6-48bd-83a9-2fbe19d81328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cause: {'top_cause': 'Excessive Speed', 'p_top_cause': 0.7911969378894055, 'probs': {'Excessive Speed': 0.7911969378894055, 'Mechanical Error': 0.03227734344442165, 'Slipped': 0.068024801995568, 'Slipped, Excessive Speed': 0.10850091667060487}}\n",
            "Rate : 1.0366130175658315\n",
            "Risk : {'risk_0_100': 100.0, 'top_cause': 'Excessive Speed', 'p_top_cause': 0.791, 'rate_pred': 1.037, 'components': {'cause_component': 0.811, 'rate_component': 1.0, 'S_vehicle': 1.0, 'W_weather': 1.25}}\n"
          ]
        }
      ]
    }
  ]
}