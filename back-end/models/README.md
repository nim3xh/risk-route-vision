# XGBoost Model Integration

## Overview

The Risk Route Vision backend now uses a trained XGBoost model for accurate risk prediction with vehicle-specific thresholds.

## Required Model Files

Place these files in the `back-end/models/` directory:

1. **`xgb_vehicle_specific_risk.pkl`** - The trained XGBoost pipeline
   - Generated by running the training notebook (`XG_BOOST_NEWER.ipynb`)
   - Contains the full preprocessing pipeline and XGBoost regressor
   
2. **`vehicle_thresholds.csv`** - Vehicle-specific risk thresholds
   - Format:
     ```csv
     Vehicle,threshold
     __GLOBAL__,0.5
     MOTORCYCLE,0.45
     THREE_WHEELER,0.48
     CAR,0.50
     BUS,0.55
     LORRY,0.52
     ```

## How to Train the Model

### 1. Prepare Your Dataset

Your dataset should include columns like:
- `Date`, `Time`, or `Datetime` - Timestamp information
- `Latitude`, `Longitude` - Location coordinates
- `Temperature (C)`, `Humidity (%)`, `Precipitation (mm)`, `Wind Speed (km/h)` - Weather
- `Vehicle` - Vehicle type (MOTORCYCLE, THREE_WHEELER, CAR, BUS, LORRY)
- `SPI_smoothed` - Target variable (Safety Performance Index)
- Other features: `Place`, `Reason`, `Position`, etc.

### 2. Run the Training Notebook

Upload `XG_BOOST_NEWER.ipynb` to Google Colab or Jupyter:

```python
# The notebook will:
# 1. Load your dataset
# 2. Engineer features (hour, dow, lat_bin, lon_bin, etc.)
# 3. Train XGBoost model with hyperparameter tuning
# 4. Generate vehicle-specific thresholds
# 5. Save model and thresholds to /content/models/
```

### 3. Download Model Files

After training completes, download these files from Colab:
- `/content/models/xgb_vehicle_specific_risk.pkl`
- `/content/outputs/vehicle_thresholds.csv`

### 4. Place Files in Backend

```
risk-route-vision/
└── back-end/
    └── models/
        ├── xgb_vehicle_specific_risk.pkl
        ├── vehicle_thresholds.csv
        └── README.md (this file)
```

## Model Features

The XGBoost model uses these features for prediction:

### Weather Features
- `Temperature (C)` - Temperature in Celsius
- `Humidity (%)` - Humidity percentage
- `Precipitation (mm)` - Precipitation amount
- `Wind Speed (km/h)` - Wind speed
- `is_wet` - Binary wet road indicator

### Location Features
- `Latitude` - GPS latitude
- `Longitude` - GPS longitude
- `lat_bin` - Latitude grid cell (lat * 100)
- `lon_bin` - Longitude grid cell (lon * 100)

### Time Features
- `hour` - Hour of day (0-23)
- `dow` - Day of week (0=Monday, 6=Sunday)
- `is_weekend` - Weekend flag
- `timestamp` - Unix timestamp

### Vehicle Feature
- `Vehicle` - Vehicle type (categorical)

### Derived Features
- `curvature` - Road curvature estimate (if available)
- `is_speed_reason` - Speed-related incident flag (if in dataset)

## Vehicle-Specific Thresholds

The model applies different risk thresholds for each vehicle type:

| Vehicle | Multiplier | Threshold | Risk Level |
|---------|-----------|-----------|------------|
| Motorcycle | 1.3x | Lower | Higher risk |
| Three Wheeler | 1.15x | Lower | High risk |
| Car | 1.0x | Baseline | Normal |
| Van | 1.0x | Baseline | Normal |
| Bus | 0.85x | Higher | Lower risk |
| Lorry | 0.90x | Higher | Lower risk |

**Why different thresholds?**
- Motorcycles are inherently more vulnerable → lower threshold for alerts
- Buses are more stable → higher threshold needed
- Ensures fair risk assessment across vehicle types

## How the Model Works

### 1. Feature Preparation
```python
# Backend automatically prepares features from:
- Route coordinates
- Weather data (from Open-Meteo API)
- Vehicle type
- Current timestamp
```

### 2. Prediction
```python
# Model predicts SPI (Safety Performance Index)
spi_pred = model.predict(features)  # Returns 0.0 - 1.0

# Apply vehicle-specific threshold
threshold = vehicle_thresholds[vehicle_type]
is_high_risk = spi_pred >= threshold

# Normalize to risk score
risk_score = normalize(spi_pred, threshold)  # 0.0 - 1.0
```

### 3. Risk Causes
```python
# Generate contextual risk cause based on:
- Risk level (high/medium/low)
- Vehicle type
- Weather conditions
- Location characteristics
```

## Model Performance

Based on training notebook metrics:

### Regression Metrics (SPI Prediction)
- **R²**: ~0.65-0.85 (varies by dataset)
- **MAE**: ~0.05-0.10
- **RMSE**: ~0.08-0.15

### Classification Metrics (High-Risk Alert)
- **Accuracy**: ~75-85%
- **Precision**: ~70-80%
- **Recall**: ~75-85%
- **F1-Score**: ~72-82%

*Actual metrics depend on your training data quality and size*

## API Integration

### Endpoint: POST `/api/v1/risk/score`

**Request:**
```json
{
  "vehicleType": "MOTORCYCLE",
  "coordinates": [[6.9893, 80.4927], [6.9900, 80.4935]],
  "timestampUtc": "2025-11-12T08:30:00Z"
}
```

**Response:**
```json
{
  "overall": 0.75,
  "segmentScores": [0.72, 0.78],
  "explain": {
    "curvature": 0.15,
    "surface_wetness_prob": 0.0,
    "wind_speed": 12.5,
    "temperature": 28.5,
    "vehicle_factor": 1.2
  }
}
```

## Fallback Behavior

If model files are not found:
1. Backend uses **dummy predictions** based on simple heuristics
2. Console warning: `[Warn] Using dummy predictions - model not loaded`
3. Risk calculation: `0.15*vehicle_factor + 0.7*curvature + 0.15*wetness`

## Environment Configuration

### Option 1: Default Location (Recommended)
```bash
# Place files in back-end/models/
# No configuration needed
```

### Option 2: Custom Location
```bash
# Set environment variable
export RISK_MODEL_PATH="/path/to/xgb_vehicle_specific_risk.pkl"

# Or in .env file:
RISK_MODEL_PATH=/path/to/xgb_vehicle_specific_risk.pkl
```

## Testing the Model

### 1. Check Model Loaded
```bash
# Start backend
cd back-end
uvicorn app.main:app --reload

# Check console for:
# [Info] Loaded XGBoost model from /path/to/model
# [Info] Loaded vehicle thresholds from /path/to/thresholds.csv
```

### 2. Test Prediction API
```bash
curl -X POST http://localhost:8080/api/v1/risk/score \
  -H "Content-Type: application/json" \
  -d '{
    "vehicleType": "MOTORCYCLE",
    "coordinates": [[6.9893, 80.4927]],
    "timestampUtc": null
  }'
```

### 3. Verify Different Vehicle Risks
```bash
# Test each vehicle type
for vehicle in MOTORCYCLE CAR BUS; do
  echo "Testing $vehicle..."
  curl -X POST http://localhost:8080/api/v1/risk/score \
    -H "Content-Type: application/json" \
    -d "{\"vehicleType\":\"$vehicle\",\"coordinates\":[[6.9893,80.4927]]}"
done
```

## Troubleshooting

### Model Not Loading
```
[Warn] Model file not found at /path/to/model, using dummy predictions
```
**Solution:** 
- Verify model file exists in `back-end/models/`
- Check file permissions
- Ensure file is not corrupted

### Import Errors
```
ModuleNotFoundError: No module named 'xgboost'
```
**Solution:**
```bash
cd back-end
pip install xgboost scikit-learn joblib pandas numpy
```

### Threshold File Missing
```
[Warn] Thresholds file not found, using defaults
```
**Solution:**
- Place `vehicle_thresholds.csv` in `back-end/models/`
- Or system will use default thresholds

### Prediction Errors
```
[Error] Prediction failed: [error message]
```
**Solution:**
- Check feature preparation matches training
- Verify input data types
- Review model compatibility

## Updating the Model

### When to Retrain:
1. **New incident data** available
2. **Seasonal patterns** change
3. **Performance degradation** observed
4. **New vehicle types** added
5. **Geographic expansion** (new areas)

### How to Update:
1. Collect new data in same format
2. Re-run training notebook
3. Download new model files
4. Replace old files in `back-end/models/`
5. Restart backend server

### Version Control:
```bash
# Keep track of model versions
back-end/models/
├── xgb_vehicle_specific_risk_v1.pkl
├── xgb_vehicle_specific_risk_v2.pkl  (current)
├── vehicle_thresholds_v1.csv
└── vehicle_thresholds_v2.csv  (current)
```

## Model Explainability

The model provides:

1. **Feature Importances** - Which features matter most
2. **SHAP Values** - Individual prediction explanations
3. **Risk Causes** - Human-readable risk descriptions
4. **Threshold Transparency** - Vehicle-specific cutoffs

## Performance Optimization

### For Production:
1. **Model Caching** - Model loaded once at startup
2. **Threshold Caching** - Thresholds cached in memory
3. **Batch Predictions** - Process multiple segments efficiently
4. **Feature Reuse** - Weather data cached per request

### Expected Latency:
- First prediction: ~100-200ms (model loading)
- Subsequent predictions: ~10-50ms
- Weather API call: ~100-300ms

## Security Notes

- Model file is not checked into git (too large)
- Add to `.gitignore`: `*.pkl`, `*.joblib`
- Store models separately in production
- Consider model encryption for sensitive deployments

## Support

For issues or questions:
1. Check console logs for error messages
2. Verify all dependencies installed
3. Ensure model files in correct location
4. Review training notebook for feature compatibility
5. Test with dummy predictions first

## Related Files

- Training notebook: `XG_BOOST_NEWER.ipynb`
- Test notebook: `research.ipynb`
- Model service: `back-end/app/ml/model.py`
- Feature engineering: `back-end/app/services/feature_engineering.py`
- Risk endpoints: `back-end/app/routers/risk.py`
