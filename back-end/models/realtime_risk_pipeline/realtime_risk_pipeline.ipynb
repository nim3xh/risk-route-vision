{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IcfbGOQEGvL6",
        "outputId": "6d09da7f-e0e3-4c76-9056-88da9473af39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Please upload your final_dataset.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1695e3c-c7ef-49b5-b164-d7ab9f1fc985\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1695e3c-c7ef-49b5-b164-d7ab9f1fc985\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final_dataset.csv to final_dataset.csv\n",
            "Loaded: /content/final_dataset.csv\n",
            "Shape: (315, 24)\n",
            "Columns: ['Date', 'Time', 'Vehicle', 'Place', 'Reason', 'Position', 'Description', 'Datetime', 'Temperature (C)', 'Humidity (%)', 'Precipitation (mm)', 'Wind Speed (km/h)', 'Latitude', 'Longitude', 'ts', 'hour', 'dow', 'is_weekend', 'is_wet', 'lat_bin', 'lon_bin', 'segment_id', 'is_speed_reason', 'SPI_smoothed']\n",
            "[Info] Rows after dropping missing target: 315\n",
            "\n",
            "Selected feature groups:\n",
            "  Text: ['Description']\n",
            "  Categorical: ['Vehicle', 'Place', 'Reason', 'Position', 'segment_id']\n",
            "  Numeric: ['Temperature (C)', 'Humidity (%)', 'Precipitation (mm)', 'Wind Speed (km/h)', 'Latitude', 'Longitude', 'hour', 'dow', 'is_weekend', 'is_wet', 'lat_bin', 'lon_bin', 'is_speed_reason', 'timestamp']\n",
            "\n",
            "Train size: 252 | Test size: 63\n",
            "Train time span: 2020-01-06 08:00:00 -> 2023-11-29 07:20:00\n",
            "Test  time span: 2023-12-06 14:35:00 -> 2025-03-28 15:40:00\n",
            "\n",
            "[Info] Starting randomized hyperparameter search...\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "\n",
            "[Info] Best CV R^2: 0.794102151541391\n",
            "[Info] Best Params: {'xgb__subsample': 0.8, 'xgb__reg_lambda': 0.5, 'xgb__reg_alpha': 0.1, 'xgb__n_estimators': 500, 'xgb__min_child_weight': 6.5, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.18000000000000002, 'xgb__colsample_bytree': 0.9}\n",
            "\n",
            "=== Test Metrics (SPI_smoothed regression, XGBoost) ===\n",
            "R^2   : 0.652261\n",
            "MAE   : 0.009836\n",
            "RMSE  : 0.014979\n",
            "\n",
            "=== Test Classification Metrics (thresholded SPI for alerts; per-vehicle) ===\n",
            "Per-vehicle thresholds used (see vehicle_thresholds.csv); global fallback = 0.350718\n",
            "Accuracy : 0.761905\n",
            "Precision: 0.900000\n",
            "Recall   : 0.765957\n",
            "F1-score : 0.827586\n",
            "\n",
            "Per-vehicle alert metrics saved to: /content/outputs/classification_metrics_per_vehicle.csv\n",
            "\n",
            "[Info] Model saved to: /content/models/xgb_vehicle_specific_risk.pkl\n",
            "[Info] Metrics saved to: /content/outputs/metrics.json\n",
            "[Info] Classification metrics (overall) saved to: /content/outputs/classification_metrics.json\n",
            "[Info] Predictions saved to: /content/outputs/predictions.csv\n",
            "[Info] Per-vehicle classification metrics saved to: classification_metrics_per_vehicle.json\n",
            "\n",
            "Example prediction:\n",
            "  Vehicle : Motor Cycle\n",
            "  Place   : Ambagamuwa Temple\n",
            "  Reason  : Slipped\n",
            "  Position: 7.0189, 80.4938\n",
            "  Time    : 2023-12-06 14:35:00\n",
            "  Predicted SPI_smoothed: 0.352798\n",
            "  Actual    SPI_smoothed: 0.350718\n",
            "  Thr used : 0.398337\n",
            "  HighRisk (pred/true): 0 / 0\n",
            "  Temperature (C): 24.8\n",
            "  Humidity (%): 89\n",
            "  Precipitation (mm): 0.2\n",
            "  Wind Speed (km/h): 7.6\n",
            "  hour: 14\n",
            "  dow: 2\n",
            "  is_wet: 1\n",
            "\n",
            "Example prediction:\n",
            "  Vehicle : Van\n",
            "  Place   : Diyagala 56/8 Culvert\n",
            "  Reason  : Excessive Speed\n",
            "  Position: 6.979536, 80.503648\n",
            "  Time    : 2023-12-13 21:00:00\n",
            "  Predicted SPI_smoothed: 0.413832\n",
            "  Actual    SPI_smoothed: 0.398337\n",
            "  Thr used : 0.398337\n",
            "  HighRisk (pred/true): 1 / 1\n",
            "  Temperature (C): 22.3\n",
            "  Humidity (%): 91\n",
            "  Precipitation (mm): 0.0\n",
            "  Wind Speed (km/h): 1.6\n",
            "  hour: 21\n",
            "  dow: 2\n",
            "  is_wet: 0\n",
            "\n",
            "Example prediction:\n",
            "  Vehicle : Three Wheeler\n",
            "  Place   : Ginigathena Hospital\n",
            "  Reason  : Excessive Speed\n",
            "  Position: 6.9924, 80.4886\n",
            "  Time    : 2023-12-18 13:25:00\n",
            "  Predicted SPI_smoothed: 0.403113\n",
            "  Actual    SPI_smoothed: 0.398337\n",
            "  Thr used : 0.350718\n",
            "  HighRisk (pred/true): 1 / 1\n",
            "  Temperature (C): 24.2\n",
            "  Humidity (%): 91\n",
            "  Precipitation (mm): 8.8\n",
            "  Wind Speed (km/h): 5.4\n",
            "  hour: 13\n",
            "  dow: 0\n",
            "  is_wet: 1\n",
            "\n",
            "=== DONE ===\n",
            "Outputs directory: /content/outputs\n",
            "Model directory  : /content/models\n"
          ]
        }
      ],
      "source": [
        "!pip -q install xgboost shap\n",
        "\n",
        "import os, re, json, warnings, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_absolute_error, mean_squared_error,\n",
        "    accuracy_score, precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.model_selection import TimeSeriesSplit, KFold, RandomizedSearchCV, learning_curve\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from xgboost import XGBRegressor\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "# CONFIG\n",
        "RANDOM_STATE = 42\n",
        "random.seed(RANDOM_STATE)\n",
        "\n",
        "TARGET = \"SPI_smoothed\"\n",
        "DEFAULT_CSV_PATH = \"/content/final_dataset.csv\"\n",
        "\n",
        "OUT_DIR = \"/content/outputs\"\n",
        "MODEL_DIR = \"/content/models\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "DO_TUNE = True\n",
        "N_ITER_TUNE = 20\n",
        "CV_SPLITS = 5\n",
        "SHAP_SAMPLE = 2000\n",
        "HASH_DIM = 512\n",
        "TOP_FEATS = 30\n",
        "\n",
        "# Strategy to convert continuous SPI -> HighRisk=True\n",
        "CLASS_THRESHOLD_STRATEGY = \"median\"   # median\n",
        "CLASS_THRESHOLD_Q = 0.5\n",
        "\n",
        "# per-vehicle thresholds\n",
        "CLASS_THRESHOLD_PER_VEHICLE = True\n",
        "MIN_TRAIN_RECORDS_PER_VEHICLE = 8     # if fewer than this in training, fall back to GLOBAL\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"This Pipeline instance is not fitted yet\",\n",
        "    category=FutureWarning\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"The NumPy global RNG was seeded by calling `np.random.seed`\",\n",
        "    category=FutureWarning\n",
        ")\n",
        "\n",
        "# UTILS\n",
        "def build_timestamp(df: pd.DataFrame) -> pd.Series:\n",
        "    ts = None\n",
        "    if 'ts' in df.columns:\n",
        "        t = pd.to_datetime(df['ts'], errors=\"coerce\")\n",
        "        if t.notna().any(): ts = t\n",
        "    if ts is None and 'Datetime' in df.columns:\n",
        "        t = pd.to_datetime(df['Datetime'], errors=\"coerce\")\n",
        "        if t.notna().any(): ts = t\n",
        "    if ts is None and ('Date' in df.columns or 'Time' in df.columns):\n",
        "        if 'Date' in df.columns and 'Time' in df.columns:\n",
        "            t = pd.to_datetime(df['Date'].astype(str) + \" \" + df['Time'].astype(str), errors=\"coerce\")\n",
        "        else:\n",
        "            t = pd.to_datetime(df.get('Date', pd.NaT), errors=\"coerce\")\n",
        "        ts = t\n",
        "    if ts is None:\n",
        "        ts = pd.Series([pd.NaT]*len(df), index=df.index)\n",
        "    return (ts.astype('int64') // 10**9).astype('float')\n",
        "\n",
        "def drop_slope_elevation(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    to_drop = [c for c in df.columns if ('slope' in c.lower()) or ('elev' in c.lower())]\n",
        "    if to_drop:\n",
        "        print(f\"[Info] Dropping slope/elevation columns: {to_drop}\")\n",
        "    return df.drop(columns=to_drop, errors=\"ignore\")\n",
        "\n",
        "class DenseHashingVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=HASH_DIM, input_col='text'):\n",
        "        self.n_features = int(n_features)\n",
        "        self.input_col = input_col\n",
        "        self.vec = HashingVectorizer(n_features=self.n_features, alternate_sign=False, norm=None)\n",
        "    def fit(self, X, y=None): return self\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame): X = X.iloc[:, 0]\n",
        "        X = pd.Series(X).fillna(\"\").astype(str).values\n",
        "        return self.vec.transform(X).toarray()\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        base = re.sub(r'\\W+', '_', str(self.input_col).lower())\n",
        "        return np.array([f\"{base}_hash_{i}\" for i in range(self.n_features)])\n",
        "\n",
        "def choose_feature_columns(df: pd.DataFrame, target: str) -> Tuple[List[str], List[str], List[str]]:\n",
        "    cols = df.columns.tolist()\n",
        "    # text\n",
        "    text_cols = [c for c in cols if c.lower() == 'description' and c != target]\n",
        "    # categorical\n",
        "    cat_cols = [c for c in cols if df[c].dtype == 'object' and c not in text_cols]\n",
        "    for c in ['Date', 'Time', 'Datetime', 'ts']:\n",
        "        if c in cat_cols:\n",
        "            cat_cols.remove(c)\n",
        "    # numeric (include ALL numeric columns)\n",
        "    num_cols = [c for c in cols if np.issubdtype(df[c].dtype, np.number) and c != target]\n",
        "    return text_cols, cat_cols, num_cols\n",
        "\n",
        "def get_feature_names_from_ct(ct: ColumnTransformer) -> List[str]:\n",
        "    names = []\n",
        "    for name, transformer, cols in ct.transformers_:\n",
        "        if transformer == \"drop\" or cols is None or len(cols) == 0: continue\n",
        "        if isinstance(transformer, Pipeline):\n",
        "            if name == \"text\":\n",
        "                step = transformer.named_steps.get(\"hash\")\n",
        "                try: names.extend(step.get_feature_names_out())\n",
        "                except: names.extend([f\"text_hash_{i}\" for i in range(HASH_DIM)])\n",
        "                continue\n",
        "            if name == \"cat\" and \"onehot\" in transformer.named_steps:\n",
        "                ohe = transformer.named_steps[\"onehot\"]\n",
        "                try: ohe_names = ohe.get_feature_names_out(cols)\n",
        "                except:\n",
        "                    try: ohe_names = ohe.get_feature_names(cols)\n",
        "                    except:\n",
        "                        ohe_names = []\n",
        "                        if hasattr(ohe,\"categories_\"):\n",
        "                            for c, cat_vals in zip(cols, ohe.categories_):\n",
        "                                for v in cat_vals: ohe_names.append(f\"{c}_{v}\")\n",
        "                        else:\n",
        "                            ohe_names = [f\"cat__{c}\" for c in cols]\n",
        "                names.extend(list(ohe_names)); continue\n",
        "            if name == \"num\":\n",
        "                names.extend([f\"{c}\" for c in cols]); continue\n",
        "            names.extend([f\"{name}__{c}\" for c in cols])\n",
        "        else:\n",
        "            try:\n",
        "                direct_names = transformer.get_feature_names_out(cols)\n",
        "                names.extend(direct_names.tolist())\n",
        "            except:\n",
        "                names.extend([f\"{name}__{c}\" for c in cols])\n",
        "    return names\n",
        "\n",
        "def safe_show(figpath: str, title: Optional[str] = None):\n",
        "    try: plt.tight_layout()\n",
        "    except: pass\n",
        "    plt.savefig(figpath, bbox_inches=\"tight\", dpi=150); plt.close()\n",
        "    img = plt.imread(figpath); plt.figure(figsize=(8,6)); plt.imshow(img); plt.axis('off')\n",
        "    if title: plt.title(title); plt.show()\n",
        "\n",
        "def compute_class_threshold(y_vals: np.ndarray, strategy: str = \"median\", q: float = 0.5) -> float:\n",
        "    y_vals = np.asarray(y_vals, dtype=float)\n",
        "    if strategy == \"median\":\n",
        "        return float(np.median(y_vals))\n",
        "    elif strategy == \"quantile\":\n",
        "        q = float(min(max(q, 0.0), 1.0))\n",
        "        return float(np.quantile(y_vals, q))\n",
        "    else:\n",
        "        raise ValueError(\"CLASS_THRESHOLD_STRATEGY must be 'median' or 'quantile'\")\n",
        "\n",
        "def compute_vehicle_thresholds(train_df: pd.DataFrame,\n",
        "                               strategy: str,\n",
        "                               q: float,\n",
        "                               min_n: int,\n",
        "                               target_col: str = TARGET,\n",
        "                               vehicle_col: str = \"Vehicle\") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Returns a dict {vehicle -> threshold}. Vehicles with < min_n samples\n",
        "    (or missing vehicle) fall back to the GLOBAL threshold under key '__GLOBAL__'.\n",
        "    \"\"\"\n",
        "    thr_global = compute_class_threshold(train_df[target_col].values, strategy, q)\n",
        "    out = {\"__GLOBAL__\": thr_global}\n",
        "    if vehicle_col not in train_df.columns:\n",
        "        return out\n",
        "\n",
        "    grp = train_df.dropna(subset=[target_col]).groupby(vehicle_col)\n",
        "    for v, g in grp:\n",
        "        if len(g) >= min_n:\n",
        "            out[str(v)] = compute_class_threshold(g[target_col].values, strategy, q)\n",
        "    return out\n",
        "\n",
        "def apply_vehicle_thresholds(df_like: pd.DataFrame,\n",
        "                             preds: np.ndarray,\n",
        "                             thresholds: Dict[str, float],\n",
        "                             vehicle_col: str = \"Vehicle\",\n",
        "                             target_col: str = TARGET) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Produces binary true/pred arrays using per-vehicle thresholds.\n",
        "    Returns (y_true_bin, y_pred_bin, used_thresholds_per_row)\n",
        "    \"\"\"\n",
        "    thr_global = thresholds.get(\"__GLOBAL__\", compute_class_threshold(df_like[target_col].values, \"median\", 0.5))\n",
        "    veh_series = df_like[vehicle_col].astype(str) if vehicle_col in df_like.columns else pd.Series([\"__NA__\"]*len(df_like), index=df_like.index)\n",
        "    used_thr = np.array([thresholds.get(v, thr_global) for v in veh_series])\n",
        "    y_true_bin = (df_like[target_col].values >= used_thr).astype(int)\n",
        "    y_pred_bin = (preds >= used_thr).astype(int)\n",
        "    return y_true_bin, y_pred_bin, used_thr\n",
        "\n",
        "# --------------------------\n",
        "# LOAD\n",
        "# --------------------------\n",
        "csv_path = DEFAULT_CSV_PATH\n",
        "if not os.path.exists(csv_path):\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        print(\"[Info] Please upload your final_dataset.csv\")\n",
        "        uploaded = files.upload()\n",
        "        for k in uploaded.keys():\n",
        "            if k.lower().endswith(\".csv\"): csv_path = f\"/content/{k}\"; break\n",
        "        assert os.path.exists(csv_path), \"No CSV file detected. Please upload your dataset.\"\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(\"Upload your CSV via the Colab file widget or set csv_path.\") from e\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"Loaded: {csv_path}\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# CLEAN\n",
        "df = drop_slope_elevation(df)\n",
        "\n",
        "speed_like = [c for c in df.columns if (\"speed\" in c.lower())]\n",
        "for c in speed_like:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "df[\"timestamp\"] = build_timestamp(df)\n",
        "\n",
        "num_like = [\n",
        "    'Temperature (C)','Humidity (%)','Precipitation (mm)','Wind Speed (km/h)',\n",
        "    'Latitude','Longitude','hour','dow','is_weekend','is_wet','lat_bin','lon_bin',\n",
        "    'is_speed_reason','timestamp'\n",
        "]\n",
        "for col in num_like:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target '{TARGET}' not found in dataset.\")\n",
        "df = df[~df[TARGET].isna()].copy()\n",
        "print(f\"[Info] Rows after dropping missing target: {df.shape[0]}\")\n",
        "\n",
        "missing = df.isna().mean().sort_values(ascending=False).head(25)\n",
        "plt.figure(figsize=(10,6)); missing.plot(kind=\"bar\")\n",
        "plt.title(\"Top-25 Missingness by Column\"); plt.ylabel(\"Fraction Missing\")\n",
        "safe_show(os.path.join(OUT_DIR, \"missingness_top25.png\"))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(df[TARGET].dropna().values, bins=40)\n",
        "plt.title(f\"Distribution of {TARGET}\"); plt.xlabel(TARGET); plt.ylabel(\"Count\")\n",
        "safe_show(os.path.join(OUT_DIR, f\"dist_{TARGET}.png\"))\n",
        "\n",
        "if \"hour\" in df.columns:\n",
        "    plt.figure(figsize=(9,5))\n",
        "    df[\"hour\"].dropna().astype(int).value_counts().sort_index().plot(kind=\"bar\")\n",
        "    plt.title(\"Incidents by Hour of Day\"); plt.xlabel(\"Hour\"); plt.ylabel(\"Count\")\n",
        "    safe_show(os.path.join(OUT_DIR, \"incidents_by_hour.png\"))\n",
        "\n",
        "if \"dow\" in df.columns:\n",
        "    plt.figure(figsize=(9,5))\n",
        "    df[\"dow\"].dropna().astype(int).value_counts().sort_index().plot(kind=\"bar\")\n",
        "    plt.title(\"Incidents by Day of Week (0=Mon ... 6=Sun)\"); plt.xlabel(\"Day\"); plt.ylabel(\"Count\")\n",
        "    safe_show(os.path.join(OUT_DIR, \"incidents_by_dow.png\"))\n",
        "\n",
        "if (\"Longitude\" in df.columns) and (\"Latitude\" in df.columns):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(df[\"Longitude\"], df[\"Latitude\"], s=8, alpha=0.8)\n",
        "    plt.title(\"Incident Locations (Longitude vs Latitude)\")\n",
        "    plt.xlabel(\"Longitude\"); plt.ylabel(\"Latitude\")\n",
        "    safe_show(os.path.join(OUT_DIR, \"geo_scatter.png\"))\n",
        "\n",
        "num_cols_all = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
        "corr_subset = [c for c in num_cols_all if c in [\n",
        "    'Temperature (C)','Humidity (%)','Precipitation (mm)','Wind Speed (km/h)',\n",
        "    'Latitude','Longitude','hour','dow','is_weekend','is_wet','lat_bin','lon_bin',\n",
        "    'is_speed_reason','timestamp', TARGET\n",
        "]]\n",
        "if len(corr_subset) >= 3:\n",
        "    corr = df[corr_subset].corr(numeric_only=True)\n",
        "    plt.figure(figsize=(9,7))\n",
        "    im = plt.imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(len(corr_subset)), corr_subset, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(corr_subset)), corr_subset)\n",
        "    plt.title(\"Correlation Heatmap (subset)\")\n",
        "    safe_show(os.path.join(OUT_DIR, \"correlation_heatmap.png\"))\n",
        "\n",
        "# FEATURES + PREPROCESS\n",
        "text_cols, cat_cols, num_cols = choose_feature_columns(df, TARGET)\n",
        "print(\"\\nSelected feature groups:\")\n",
        "print(\"  Text:\", text_cols if text_cols else \"None\")\n",
        "print(\"  Categorical:\", cat_cols if cat_cols else \"None\")\n",
        "print(\"  Numeric:\", num_cols if num_cols else \"None\")\n",
        "\n",
        "text_transformer = Pipeline(steps=[\n",
        "    (\"hash\", DenseHashingVectorizer(n_features=HASH_DIM,\n",
        "                                    input_col=text_cols[0] if text_cols else \"text\"))\n",
        "]) if text_cols else \"drop\"\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "]) if cat_cols else \"drop\"\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
        "]) if num_cols else \"drop\"\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"text\", text_transformer, text_cols),\n",
        "        (\"cat\" , categorical_transformer, cat_cols),\n",
        "        (\"num\" , numeric_transformer, num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# SPLIT (chronological)\n",
        "use_time_split = df[\"timestamp\"].notna().any()\n",
        "if use_time_split: df = df.sort_values(\"timestamp\")\n",
        "else: print(\"[Warn] No usable timestamp found; using order-based split.\")\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df.iloc[:train_size].copy()\n",
        "test_df  = df.iloc[train_size:].copy()\n",
        "\n",
        "X_train = train_df[text_cols + cat_cols + num_cols]\n",
        "y_train = train_df[TARGET].values\n",
        "X_test  = test_df[text_cols + cat_cols + num_cols]\n",
        "y_test  = test_df[TARGET].values\n",
        "\n",
        "print(f\"\\nTrain size: {len(X_train)} | Test size: {len(X_test)}\")\n",
        "if use_time_split:\n",
        "    print(f\"Train time span: {pd.to_datetime(train_df['timestamp'], unit='s', errors='coerce').min()} -> \"\n",
        "          f\"{pd.to_datetime(train_df['timestamp'], unit='s', errors='coerce').max()}\")\n",
        "    print(f\"Test  time span: {pd.to_datetime(test_df['timestamp'], unit='s', errors='coerce').min()} -> \"\n",
        "          f\"{pd.to_datetime(test_df['timestamp'], unit='s', errors='coerce').max()}\")\n",
        "\n",
        "# MODEL + TUNING\n",
        "base_xgb = XGBRegressor(\n",
        "    n_estimators=700, learning_rate=0.05, max_depth=6,\n",
        "    subsample=0.8, colsample_bytree=0.8,\n",
        "    reg_lambda=1.0, reg_alpha=0.0, min_child_weight=1.0,\n",
        "    objective=\"reg:squarederror\", tree_method=\"hist\",\n",
        "    importance_type=\"gain\", random_state=RANDOM_STATE, n_jobs=-1\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[(\"pre\", preprocessor), (\"xgb\", base_xgb)])\n",
        "\n",
        "if DO_TUNE:\n",
        "    print(\"\\n[Info] Starting randomized hyperparameter search...\")\n",
        "    cv = TimeSeriesSplit(n_splits=CV_SPLITS) if use_time_split else KFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    param_distributions = {\n",
        "        \"xgb__n_estimators\":   np.linspace(300, 1200, 10, dtype=int),\n",
        "        \"xgb__learning_rate\":  np.linspace(0.01, 0.2, 20),\n",
        "        \"xgb__max_depth\":      np.arange(3, 11),\n",
        "        \"xgb__subsample\":      np.linspace(0.6, 1.0, 9),\n",
        "        \"xgb__colsample_bytree\": np.linspace(0.6, 1.0, 9),\n",
        "        \"xgb__min_child_weight\": np.linspace(0.5, 8.0, 16),\n",
        "        \"xgb__reg_lambda\":     np.linspace(0.0, 5.0, 11),\n",
        "        \"xgb__reg_alpha\":      np.linspace(0.0, 1.0, 11),\n",
        "    }\n",
        "    rs = RandomizedSearchCV(\n",
        "        estimator=pipeline, param_distributions=param_distributions,\n",
        "        n_iter=N_ITER_TUNE, scoring=\"r2\", cv=cv, verbose=1,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1\n",
        "    )\n",
        "    rs.fit(X_train, y_train)\n",
        "    best_model = rs.best_estimator_\n",
        "    clean_params = {k: (v.item() if hasattr(v, \"item\") else v) for k, v in rs.best_params_.items()}\n",
        "    print(\"\\n[Info] Best CV R^2:\", float(rs.best_score_))\n",
        "    print(\"[Info] Best Params:\", clean_params)\n",
        "else:\n",
        "    print(\"\\n[Info] Training base XGBoost model (no tuning)...\")\n",
        "    best_model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "# EVALUATION (regression)\n",
        "preds = best_model.predict(X_test)\n",
        "r2  = float(r2_score(y_test, preds))\n",
        "mae = float(mean_absolute_error(y_test, preds))\n",
        "rmse = float(np.sqrt(mean_squared_error(y_test, preds)))\n",
        "\n",
        "print(\"\\n=== Test Metrics (SPI_smoothed regression, XGBoost) ===\")\n",
        "print(f\"R^2   : {r2:.6f}\")\n",
        "print(f\"MAE   : {mae:.6f}\")\n",
        "print(f\"RMSE  : {rmse:.6f}\")\n",
        "\n",
        "# Residuals vs Pred\n",
        "residuals = y_test - preds\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(preds, residuals, s=12, alpha=0.7)\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "plt.xlabel(\"Predicted SPI_smoothed\"); plt.ylabel(\"Residual (True - Pred)\")\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "safe_show(os.path.join(OUT_DIR, \"residuals_vs_pred.png\"))\n",
        "\n",
        "# Residuals distribution\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.hist(residuals, bins=40)\n",
        "plt.title(\"Residuals Distribution\"); plt.xlabel(\"Residual\"); plt.ylabel(\"Count\")\n",
        "safe_show(os.path.join(OUT_DIR, \"residuals_hist.png\"))\n",
        "\n",
        "# Parity plot (y = x)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, preds, s=12, alpha=0.7)\n",
        "lims = [min(y_test.min(), preds.min()), max(y_test.max(), preds.max())]\n",
        "plt.plot(lims, lims, 'k--', linewidth=1)\n",
        "plt.xlabel(\"Actual SPI_smoothed\"); plt.ylabel(\"Predicted SPI_smoothed\")\n",
        "plt.title(\"Parity Plot (Pred vs Actual)\")\n",
        "safe_show(os.path.join(OUT_DIR, \"parity_plot.png\"))\n",
        "\n",
        "# Time series residuals (if timestamp)\n",
        "if test_df[\"timestamp\"].notna().any():\n",
        "    ts_ser = pd.to_datetime(test_df[\"timestamp\"], unit='s', errors='coerce')\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(ts_ser, residuals, marker='o', linestyle='-', linewidth=1)\n",
        "    plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "    plt.title(\"Residuals over Time (Test Set)\")\n",
        "    plt.xlabel(\"Time\"); plt.ylabel(\"Residual\")\n",
        "    plt.xticks(rotation=25)\n",
        "    safe_show(os.path.join(OUT_DIR, \"residuals_over_time.png\"))\n",
        "\n",
        "# Learning curve (optional)\n",
        "max_for_lc = min(len(X_train), 5000)\n",
        "if max_for_lc >= 200:\n",
        "    X_lc = X_train.iloc[:max_for_lc]; y_lc = y_train[:max_for_lc]\n",
        "    cv_lc = TimeSeriesSplit(n_splits=CV_SPLITS) if use_time_split else KFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator=best_model, X=X_lc, y=y_lc, cv=cv_lc, scoring=\"r2\",\n",
        "        train_sizes=np.linspace(0.1, 1.0, 8), n_jobs=-1, shuffle=False\n",
        "    )\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(train_sizes, train_scores.mean(axis=1), marker=\"o\", label=\"Train R^2\")\n",
        "    plt.plot(train_sizes, val_scores.mean(axis=1), marker=\"s\", label=\"CV R^2\")\n",
        "    plt.xlabel(\"Training samples\"); plt.ylabel(\"R^2\"); plt.title(\"Learning Curve (XGBoost)\")\n",
        "    plt.legend()\n",
        "    safe_show(os.path.join(OUT_DIR, \"learning_curve.png\"))\n",
        "\n",
        "# THRESHOLDS + CLASSIFICATION METRICS (GLOBAL & PER-VEHICLE)\n",
        "# Global baseline threshold (train-only)\n",
        "thr_global = compute_class_threshold(y_train, CLASS_THRESHOLD_STRATEGY, CLASS_THRESHOLD_Q)\n",
        "\n",
        "# Per-vehicle thresholds (train-only)\n",
        "vehicle_thresholds = compute_vehicle_thresholds(\n",
        "    train_df, CLASS_THRESHOLD_STRATEGY, CLASS_THRESHOLD_Q,\n",
        "    MIN_TRAIN_RECORDS_PER_VEHICLE, TARGET, \"Vehicle\"\n",
        ")\n",
        "\n",
        "# Save thresholds\n",
        "thr_rows = [{\"Vehicle\": k, \"threshold\": v} for k, v in vehicle_thresholds.items() if k != \"__GLOBAL__\"]\n",
        "thr_rows.append({\"Vehicle\": \"__GLOBAL__\", \"threshold\": vehicle_thresholds[\"__GLOBAL__\"]})\n",
        "pd.DataFrame(thr_rows).to_csv(os.path.join(OUT_DIR, \"vehicle_thresholds.csv\"), index=False)\n",
        "\n",
        "# Apply thresholds on test set\n",
        "if CLASS_THRESHOLD_PER_VEHICLE:\n",
        "    y_test_cls, y_pred_cls, used_thr_vec = apply_vehicle_thresholds(\n",
        "        test_df, preds, vehicle_thresholds, vehicle_col=\"Vehicle\", target_col=TARGET\n",
        "    )\n",
        "    thr_used_label = \"per-vehicle\"\n",
        "else:\n",
        "    used_thr_vec = np.full_like(preds, fill_value=thr_global, dtype=float)\n",
        "    y_test_cls = (y_test >= thr_global).astype(int)\n",
        "    y_pred_cls = (preds  >= thr_global).astype(int)\n",
        "    thr_used_label = \"global\"\n",
        "\n",
        "# Overall metrics\n",
        "acc = float(accuracy_score(y_test_cls, y_pred_cls))\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_test_cls, y_pred_cls, average=\"binary\", zero_division=0)\n",
        "prec = float(prec); rec = float(rec); f1 = float(f1)\n",
        "\n",
        "print(f\"\\n=== Test Classification Metrics (thresholded SPI for alerts; {thr_used_label}) ===\")\n",
        "if thr_used_label == \"global\":\n",
        "    print(f\"Global Threshold used on SPI (train-derived): {thr_global:.6f}  (strategy={CLASS_THRESHOLD_STRATEGY}, q={CLASS_THRESHOLD_Q})\")\n",
        "else:\n",
        "    print(f\"Per-vehicle thresholds used (see vehicle_thresholds.csv); global fallback = {vehicle_thresholds['__GLOBAL__']:.6f}\")\n",
        "print(f\"Accuracy : {acc:.6f}\")\n",
        "print(f\"Precision: {prec:.6f}\")\n",
        "print(f\"Recall   : {rec:.6f}\")\n",
        "print(f\"F1-score : {f1:.6f}\")\n",
        "\n",
        "# Per-vehicle metrics table\n",
        "veh_list = test_df[\"Vehicle\"].astype(str) if \"Vehicle\" in test_df.columns else pd.Series([\"__NA__\"]*len(test_df))\n",
        "per_vehicle_rows = []\n",
        "for v in sorted(veh_list.unique()):\n",
        "    idx = (veh_list == v).values\n",
        "    if idx.sum() == 0: continue\n",
        "    acc_v = float(accuracy_score(y_test_cls[idx], y_pred_cls[idx]))\n",
        "    p_v, r_v, f_v, _ = precision_recall_fscore_support(y_test_cls[idx], y_pred_cls[idx], average=\"binary\", zero_division=0)\n",
        "    thr_v = vehicle_thresholds.get(v, vehicle_thresholds[\"__GLOBAL__\"])\n",
        "    per_vehicle_rows.append({\n",
        "        \"Vehicle\": v,\n",
        "        \"n_test\": int(idx.sum()),\n",
        "        \"threshold_used\": float(thr_v),\n",
        "        \"accuracy\": float(acc_v),\n",
        "        \"precision\": float(p_v),\n",
        "        \"recall\": float(r_v),\n",
        "        \"f1\": float(f_v),\n",
        "    })\n",
        "per_vehicle_df = pd.DataFrame(per_vehicle_rows).sort_values([\"Vehicle\"])\n",
        "per_vehicle_csv = os.path.join(OUT_DIR, \"classification_metrics_per_vehicle.csv\")\n",
        "per_vehicle_df.to_csv(per_vehicle_csv, index=False)\n",
        "print(\"\\nPer-vehicle alert metrics saved to:\", per_vehicle_csv)\n",
        "\n",
        "# FEATURE IMPORTANCE + SHAP\n",
        "ct = best_model.named_steps[\"pre\"]\n",
        "feat_names = get_feature_names_from_ct(ct)\n",
        "\n",
        "xgb_est = best_model.named_steps[\"xgb\"]\n",
        "importances = getattr(xgb_est, \"feature_importances_\", None)\n",
        "if importances is not None and len(importances) == len(feat_names):\n",
        "    order = np.argsort(importances)[::-1]\n",
        "    k = min(TOP_FEATS, len(order))\n",
        "    top_features = [(feat_names[i], float(importances[i])) for i in order[:k]]\n",
        "    pd.DataFrame(top_features, columns=[\"feature\", \"importance\"]).to_csv(\n",
        "        os.path.join(OUT_DIR, \"top_features.csv\"), index=False\n",
        "    )\n",
        "    plt.figure(figsize=(10,7))\n",
        "    names_plot = [f for f, _ in top_features[:25]][::-1]\n",
        "    vals_plot  = [v for _, v in top_features[:25]][::-1]\n",
        "    plt.barh(names_plot, vals_plot)\n",
        "    plt.title(\"Top Feature Importances (XGBoost, gain)\")\n",
        "    plt.xlabel(\"Importance (gain)\")\n",
        "    safe_show(os.path.join(OUT_DIR, \"feature_importances_top25.png\"))\n",
        "else:\n",
        "    print(\"[Warn] Could not align feature importances with names; skipping plot.\")\n",
        "\n",
        "# SHAP (sampled)\n",
        "try:\n",
        "    X_test_trans = ct.transform(X_test)\n",
        "    rng = np.random.default_rng(RANDOM_STATE)\n",
        "    if X_test_trans.shape[0] > SHAP_SAMPLE:\n",
        "        idx = rng.choice(X_test_trans.shape[0], SHAP_SAMPLE, replace=False)\n",
        "        X_shap = X_test_trans[idx]\n",
        "    else:\n",
        "        X_shap = X_test_trans\n",
        "    explainer = shap.TreeExplainer(xgb_est)\n",
        "    shap_values = explainer.shap_values(X_shap)\n",
        "    shap.summary_plot(shap_values, X_shap, feature_names=feat_names, show=False)\n",
        "    safe_show(os.path.join(OUT_DIR, \"shap_summary.png\"))\n",
        "    shap.summary_plot(shap_values, X_shap, feature_names=feat_names, plot_type=\"bar\", show=False)\n",
        "    safe_show(os.path.join(OUT_DIR, \"shap_bar.png\"))\n",
        "except Exception as e:\n",
        "    print(f\"[Warn] SHAP explanation skipped: {e}\")\n",
        "\n",
        "# SAVE ARTIFACTS\n",
        "model_path = os.path.join(MODEL_DIR, \"xgb_vehicle_specific_risk.pkl\")\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"\\n[Info] Model saved to: {model_path}\")\n",
        "\n",
        "# Core regression metrics\n",
        "metrics = {\n",
        "    \"dataset_path\": csv_path,\n",
        "    \"n_train\": int(len(X_train)),\n",
        "    \"n_test\": int(len(X_test)),\n",
        "    \"target\": TARGET,\n",
        "    \"model\": \"XGBRegressor\",\n",
        "    \"tuned\": bool(DO_TUNE),\n",
        "    \"test_metrics\": {\"r2\": r2, \"mae\": mae, \"rmse\": rmse}\n",
        "}\n",
        "with open(os.path.join(OUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(\"[Info] Metrics saved to:\", os.path.join(OUT_DIR, \"metrics.json\"))\n",
        "\n",
        "# Save overall classification metrics (for the alerting use-case)\n",
        "cls_metrics = {\n",
        "    \"threshold_mode\": \"per-vehicle\" if CLASS_THRESHOLD_PER_VEHICLE else \"global\",\n",
        "    \"global_threshold\": float(thr_global),\n",
        "    \"strategy\": CLASS_THRESHOLD_STRATEGY,\n",
        "    \"q\": CLASS_THRESHOLD_Q,\n",
        "    \"accuracy\": acc,\n",
        "    \"precision\": prec,\n",
        "    \"recall\": rec,\n",
        "    \"f1\": f1\n",
        "}\n",
        "with open(os.path.join(OUT_DIR, \"classification_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cls_metrics, f, indent=2)\n",
        "print(\"[Info] Classification metrics (overall) saved to:\", os.path.join(OUT_DIR, \"classification_metrics.json\"))\n",
        "\n",
        "# Save per-row predictions + flags + threshold used\n",
        "pred_df = test_df.copy()\n",
        "pred_df[\"SPI_true\"] = y_test\n",
        "pred_df[\"SPI_pred\"] = preds\n",
        "pred_df[\"residual\"] = pred_df[\"SPI_true\"] - pred_df[\"SPI_pred\"]\n",
        "pred_df[\"thr_used\"] = used_thr_vec\n",
        "pred_df[\"is_high_true\"] = y_test_cls\n",
        "pred_df[\"is_high_pred\"] = y_pred_cls\n",
        "keep_cols = [\"SPI_true\",\"SPI_pred\",\"residual\",\"thr_used\",\"is_high_true\",\"is_high_pred\",\n",
        "             \"Vehicle\",\"Place\",\"Reason\",\"Position\",\"segment_id\",\"timestamp\"]\n",
        "keep_cols = [c for c in keep_cols if c in pred_df.columns]\n",
        "pred_csv = os.path.join(OUT_DIR, \"predictions.csv\")\n",
        "pred_df[keep_cols].to_csv(pred_csv, index=False)\n",
        "print(\"[Info] Predictions saved to:\", pred_csv)\n",
        "\n",
        "# Save per-vehicle metrics JSON too\n",
        "with open(os.path.join(OUT_DIR, \"classification_metrics_per_vehicle.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(per_vehicle_rows, f, indent=2)\n",
        "print(\"[Info] Per-vehicle classification metrics saved to: classification_metrics_per_vehicle.json\")\n",
        "\n",
        "# One-sample preview\n",
        "if len(pred_df) > 0:\n",
        "    n_preview = 3\n",
        "    sample = pred_df.iloc[:n_preview]\n",
        "\n",
        "    for i, row in sample.iterrows():\n",
        "        print(\"\\nExample prediction:\")\n",
        "        print(\"  Vehicle :\", row.get(\"Vehicle\", \"NA\"))\n",
        "        print(\"  Place   :\", row.get(\"Place\", \"NA\"))\n",
        "        print(\"  Reason  :\", row.get(\"Reason\", \"NA\"))\n",
        "        print(\"  Position:\", row.get(\"Position\", \"NA\"))\n",
        "        print(\"  Time    :\", row.get(\"Datetime\", row.get(\"Date\", \"NA\")))\n",
        "\n",
        "        # Model outputs\n",
        "        print(\"  Predicted SPI_smoothed:\", round(float(row[\"SPI_pred\"]), 6))\n",
        "        print(\"  Actual    SPI_smoothed:\", round(float(row[\"SPI_true\"]), 6))\n",
        "        print(\"  Thr used :\", round(float(row[\"thr_used\"]), 6))\n",
        "        print(\"  HighRisk (pred/true):\", int(row[\"is_high_pred\"]), \"/\", int(row[\"is_high_true\"]))\n",
        "\n",
        "        # Optional numeric context if available\n",
        "        for feat in [\n",
        "            \"Temperature (C)\",\n",
        "            \"Humidity (%)\",\n",
        "            \"Precipitation (mm)\",\n",
        "            \"Wind Speed (km/h)\",\n",
        "            \"hour\",\n",
        "            \"dow\",\n",
        "            \"is_wet\",\n",
        "        ]:\n",
        "            if feat in row.index:\n",
        "                print(f\"  {feat}: {row[feat]}\")\n",
        "\n",
        "    print(\"\\n=== DONE ===\")\n",
        "    print(f\"Outputs directory: {OUT_DIR}\")\n",
        "    print(f\"Model directory  : {MODEL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ================================================================\n",
        "# Test saved pipeline on the SAME dataset, using the LAST 63 rows\n",
        "# - Model : /content/models/xgb_vehicle_specific_risk.pkl\n",
        "# - Data  : /content/final_dataset.csv\n",
        "# - Uses  : Per-vehicle thresholds if /content/outputs/vehicle_thresholds.csv exists,\n",
        "#           otherwise derives thresholds from the TRAIN portion (all rows except last 63).\n",
        "# - Outputs:\n",
        "#     /content/outputs/predictions_from_pkl.csv\n",
        "#     /content/outputs/classification_metrics_overall.json\n",
        "#     /content/outputs/classification_metrics_per_vehicle.csv\n",
        "# - Console preview shows a few sample rows per common vehicle type.\n",
        "# ================================================================\n",
        "\n",
        "import os, warnings\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_absolute_error, mean_squared_error,\n",
        "    accuracy_score, precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "MODEL_PATH = \"/content/models/xgb_vehicle_specific_risk.pkl\"\n",
        "CSV_PATH   = \"/content/final_dataset.csv\"\n",
        "OUT_DIR    = \"/content/outputs\"\n",
        "THR_CSV    = os.path.join(OUT_DIR, \"vehicle_thresholds.csv\")  # produced by training script\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- helpers (mirror training rules) ----------------\n",
        "def build_timestamp(df: pd.DataFrame) -> pd.Series:\n",
        "    ts = None\n",
        "    if 'ts' in df.columns:\n",
        "        t = pd.to_datetime(df['ts'], errors=\"coerce\")\n",
        "        if t.notna().any(): ts = t\n",
        "    if ts is None and 'Datetime' in df.columns:\n",
        "        t = pd.to_datetime(df['Datetime'], errors=\"coerce\")\n",
        "        if t.notna().any(): ts = t\n",
        "    if ts is None and ('Date' in df.columns or 'Time' in df.columns):\n",
        "        if 'Date' in df.columns and 'Time' in df.columns:\n",
        "            t = pd.to_datetime(df['Date'].astype(str) + \" \" + df['Time'].astype(str), errors=\"coerce\")\n",
        "        else:\n",
        "            t = pd.to_datetime(df.get('Date', pd.NaT), errors=\"coerce\")\n",
        "        ts = t\n",
        "    if ts is None:\n",
        "        ts = pd.Series([pd.NaT]*len(df), index=df.index)\n",
        "    return (ts.astype('int64') // 10**9).astype('float')\n",
        "\n",
        "def drop_slope_elevation(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    to_drop = [c for c in df.columns if ('slope' in c.lower()) or ('elev' in c.lower())]\n",
        "    return df.drop(columns=to_drop, errors=\"ignore\")\n",
        "\n",
        "def expected_input_columns_from_pipeline(pipeline) -> List[str]:\n",
        "    ct = pipeline.named_steps[\"pre\"]\n",
        "    inputs = []\n",
        "    for name, transformer, cols in ct.transformers_:\n",
        "        if transformer == \"drop\" or cols is None:\n",
        "            continue\n",
        "        if isinstance(cols, list):\n",
        "            inputs.extend(cols)\n",
        "        elif isinstance(cols, (np.ndarray, pd.Index)):\n",
        "            inputs.extend(list(cols))\n",
        "        else:\n",
        "            try:\n",
        "                inputs.append(cols)\n",
        "            except:\n",
        "                pass\n",
        "    seen, ordered = set(), []\n",
        "    for c in inputs:\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "def compute_class_threshold(y_vals: np.ndarray, strategy: str = \"median\", q: float = 0.5) -> float:\n",
        "    y_vals = np.asarray(y_vals, dtype=float)\n",
        "    if strategy == \"median\":\n",
        "        return float(np.median(y_vals))\n",
        "    q = float(min(max(q, 0.0), 1.0))\n",
        "    return float(np.quantile(y_vals, q))\n",
        "\n",
        "def compute_vehicle_thresholds_from_train(train_df: pd.DataFrame,\n",
        "                                          strategy: str = \"median\",\n",
        "                                          q: float = 0.5,\n",
        "                                          min_n: int = 8,\n",
        "                                          target_col: str = \"SPI_smoothed\",\n",
        "                                          vehicle_col: str = \"Vehicle\") -> Dict[str, float]:\n",
        "    thr_global = compute_class_threshold(train_df[target_col].values, strategy, q)\n",
        "    out = {\"__GLOBAL__\": thr_global}\n",
        "    if vehicle_col not in train_df.columns:\n",
        "        return out\n",
        "    grp = train_df.dropna(subset=[target_col]).groupby(vehicle_col)\n",
        "    for v, g in grp:\n",
        "        if len(g) >= min_n:\n",
        "            out[str(v)] = compute_class_threshold(g[target_col].values, strategy, q)\n",
        "    return out\n",
        "\n",
        "def load_vehicle_thresholds_or_build(train_df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Priority:\n",
        "      1) Load thresholds saved by training at OUT_DIR/vehicle_thresholds.csv\n",
        "      2) Else, derive from train_df (all rows except the last 63), per-vehicle with global fallback\n",
        "    \"\"\"\n",
        "    if os.path.exists(THR_CSV):\n",
        "        df_thr = pd.read_csv(THR_CSV)\n",
        "        d = {str(row[\"Vehicle\"]): float(row[\"threshold\"]) for _, row in df_thr.iterrows() if row[\"Vehicle\"] != \"__GLOBAL__\"}\n",
        "        glob = df_thr.loc[df_thr[\"Vehicle\"] == \"__GLOBAL__\", \"threshold\"]\n",
        "        d[\"__GLOBAL__\"] = float(glob.iloc[0]) if len(glob) else compute_class_threshold(train_df[\"SPI_smoothed\"].values, \"median\", 0.5)\n",
        "        print(f\"[OK] Loaded vehicle thresholds from {THR_CSV}\")\n",
        "        return d\n",
        "    print(\"[Warn] vehicle_thresholds.csv not found. Building thresholds from training portion...\")\n",
        "    return compute_vehicle_thresholds_from_train(train_df, strategy=\"median\", q=0.5, min_n=8)\n",
        "\n",
        "def apply_vehicle_thresholds(df_like: pd.DataFrame,\n",
        "                             preds: np.ndarray,\n",
        "                             thresholds: Dict[str, float],\n",
        "                             vehicle_col: str = \"Vehicle\",\n",
        "                             target_col: str = \"SPI_smoothed\") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    thr_global = thresholds.get(\"__GLOBAL__\", compute_class_threshold(df_like[target_col].values, \"median\", 0.5))\n",
        "    veh_series = df_like[vehicle_col].astype(str) if vehicle_col in df_like.columns else pd.Series([\"__NA__\"]*len(df_like), index=df_like.index)\n",
        "    used_thr = np.array([thresholds.get(v, thr_global) for v in veh_series])\n",
        "    y_true_bin = (df_like[target_col].values >= used_thr).astype(int)\n",
        "    y_pred_bin = (preds >= used_thr).astype(int)\n",
        "    return y_true_bin, y_pred_bin, used_thr\n",
        "\n",
        "# ---------------- load model ----------------\n",
        "assert os.path.exists(MODEL_PATH), f\"Model not found: {MODEL_PATH}\"\n",
        "model = joblib.load(MODEL_PATH)\n",
        "print(\"[OK] Loaded model:\", MODEL_PATH)\n",
        "\n",
        "# ---------------- load data ----------------\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        print(\"[Info] Please upload your final_dataset.csv\")\n",
        "        uploaded = files.upload()\n",
        "        for k in uploaded.keys():\n",
        "            if k.lower().endswith(\".csv\"):\n",
        "                CSV_PATH = f\"/content/{k}\"\n",
        "                break\n",
        "        assert os.path.exists(CSV_PATH), \"No CSV detected.\"\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(\"Upload your CSV or set CSV_PATH.\") from e\n",
        "\n",
        "df_raw = pd.read_csv(CSV_PATH)\n",
        "print(f\"[OK] Loaded data: {CSV_PATH} | shape={df_raw.shape}\")\n",
        "\n",
        "# ---------------- light cleaning (as in training) ----------------\n",
        "df = drop_slope_elevation(df_raw.copy())\n",
        "df[\"timestamp\"] = build_timestamp(df)\n",
        "\n",
        "num_like = [\n",
        "    'Temperature (C)','Humidity (%)','Precipitation (mm)','Wind Speed (km/h)',\n",
        "    'Latitude','Longitude','hour','dow','is_weekend','is_wet','lat_bin','lon_bin',\n",
        "    'is_speed_reason','timestamp'\n",
        "]\n",
        "for col in num_like:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "if \"SPI_smoothed\" not in df.columns:\n",
        "    raise ValueError(\"Column 'SPI_smoothed' not found in the dataset.\")\n",
        "\n",
        "# ---------------- chronological split ----------------\n",
        "if df[\"timestamp\"].notna().any():\n",
        "    df_sorted = df.sort_values(\"timestamp\")\n",
        "else:\n",
        "    print(\"[Warn] No usable timestamps; using original row order.\")\n",
        "    df_sorted = df.copy()\n",
        "\n",
        "n_test = 63\n",
        "assert len(df_sorted) >= n_test, f\"Dataset has only {len(df_sorted)} rows; need at least {n_test}.\"\n",
        "train_df = df_sorted.iloc[:-n_test].copy()\n",
        "test_df  = df_sorted.iloc[-n_test:].copy()\n",
        "print(f\"[OK] Selected last {n_test} rows as test set. Train={len(train_df)} Test={len(test_df)}\")\n",
        "\n",
        "# ---------------- align features ----------------\n",
        "expected_cols = expected_input_columns_from_pipeline(model)\n",
        "present = [c for c in expected_cols if c in test_df.columns]\n",
        "missing = [c for c in expected_cols if c not in test_df.columns]\n",
        "if missing:\n",
        "    print(\"[Warn] Missing expected columns (imputers/one-hot usually handle this):\", missing)\n",
        "\n",
        "X_test = test_df[present].copy()\n",
        "y_test = test_df[\"SPI_smoothed\"].values\n",
        "\n",
        "# ---------------- predict ----------------\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "# ---------------- regression metrics ----------------\n",
        "r2   = float(r2_score(y_test, pred))\n",
        "mae  = float(mean_absolute_error(y_test, pred))\n",
        "rmse = float(np.sqrt(mean_squared_error(y_test, pred)))\n",
        "print(\"\\n=== Regression metrics on last 63 ===\")\n",
        "print(f\"R^2   : {r2:.6f}\")\n",
        "print(f\"MAE   : {mae:.6f}\")\n",
        "print(f\"RMSE  : {rmse:.6f}\")\n",
        "\n",
        "# ---------------- thresholds + alert metrics (per-vehicle) ----------------\n",
        "vehicle_thresholds = load_vehicle_thresholds_or_build(train_df)\n",
        "\n",
        "y_true_bin, y_pred_bin, thr_used_vec = apply_vehicle_thresholds(\n",
        "    test_df, pred, vehicle_thresholds, vehicle_col=\"Vehicle\", target_col=\"SPI_smoothed\"\n",
        ")\n",
        "\n",
        "acc = float(accuracy_score(y_true_bin, y_pred_bin))\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_true_bin, y_pred_bin, average=\"binary\", zero_division=0)\n",
        "print(\"\\n=== Alert metrics (per-vehicle thresholds) on last 63 ===\")\n",
        "print(f\"Accuracy : {acc:.6f}\")\n",
        "print(f\"Precision: {float(prec):.6f}\")\n",
        "print(f\"Recall   : {float(rec):.6f}\")\n",
        "print(f\"F1-score : {float(f1):.6f}\")\n",
        "\n",
        "# Per-vehicle breakdown\n",
        "veh_series = test_df[\"Vehicle\"].astype(str) if \"Vehicle\" in test_df.columns else pd.Series([\"__NA__\"]*len(test_df), index=test_df.index)\n",
        "rows = []\n",
        "for v in sorted(veh_series.unique()):\n",
        "    idx = (veh_series == v).values\n",
        "    if idx.sum() == 0:\n",
        "        continue\n",
        "    acc_v = float(accuracy_score(y_true_bin[idx], y_pred_bin[idx]))\n",
        "    p_v, r_v, f_v, _ = precision_recall_fscore_support(y_true_bin[idx], y_pred_bin[idx], average=\"binary\", zero_division=0)\n",
        "    thr_v = vehicle_thresholds.get(v, vehicle_thresholds[\"__GLOBAL__\"])\n",
        "    rows.append({\n",
        "        \"Vehicle\": v,\n",
        "        \"n_test\": int(idx.sum()),\n",
        "        \"threshold_used\": float(thr_v),\n",
        "        \"accuracy\": float(acc_v),\n",
        "        \"precision\": float(p_v),\n",
        "        \"recall\": float(r_v),\n",
        "        \"f1\": float(f_v),\n",
        "    })\n",
        "\n",
        "per_vehicle_df = pd.DataFrame(rows).sort_values([\"n_test\",\"Vehicle\"], ascending=[False, True])\n",
        "per_vehicle_csv = os.path.join(OUT_DIR, \"classification_metrics_per_vehicle.csv\")\n",
        "per_vehicle_df.to_csv(per_vehicle_csv, index=False)\n",
        "print(\"\\n[OK] Per-vehicle metrics saved to:\", per_vehicle_csv)\n",
        "print(per_vehicle_df.to_string(index=False))\n",
        "\n",
        "# ---------------- save enriched predictions ----------------\n",
        "out_df = pd.DataFrame({\n",
        "    \"SPI_true\": y_test.astype(float),\n",
        "    \"SPI_pred\": pred.astype(float),\n",
        "    \"residual\": (y_test - pred).astype(float),\n",
        "    \"thr_used\": thr_used_vec.astype(float),\n",
        "    \"is_high_true\": y_true_bin.astype(int),\n",
        "    \"is_high_pred\": y_pred_bin.astype(int),\n",
        "})\n",
        "\n",
        "# Add time & context columns if present\n",
        "for c in [\"timestamp\", \"Datetime\", \"Date\", \"Time\",\n",
        "          \"Vehicle\",\"Place\",\"Reason\",\"Position\",\"segment_id\",\"lat_bin\",\"lon_bin\",\n",
        "          \"Latitude\",\"Longitude\",\"hour\",\"dow\",\"is_weekend\",\"is_wet\",\"is_speed_reason\"]:\n",
        "    if c in test_df.columns:\n",
        "        out_df[c] = test_df[c].values\n",
        "\n",
        "save_path = os.path.join(OUT_DIR, \"predictions_from_pkl.csv\")\n",
        "out_df.to_csv(save_path, index=False)\n",
        "print(f\"\\n[OK] Saved predictions (with thresholds & flags) to:\\n  {save_path}\")\n",
        "\n",
        "# ---------------- also save overall alert metrics ----------------\n",
        "overall_json = {\n",
        "    \"accuracy\": acc,\n",
        "    \"precision\": float(prec),\n",
        "    \"recall\": float(rec),\n",
        "    \"f1\": float(f1),\n",
        "    \"n_test\": int(len(test_df)),\n",
        "    \"note\": \"Per-vehicle thresholds with global fallback\"\n",
        "}\n",
        "import json\n",
        "with open(os.path.join(OUT_DIR, \"classification_metrics_overall.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(overall_json, f, indent=2)\n",
        "print(\"[OK] Overall alert metrics saved to: classification_metrics_overall.json\")\n",
        "\n",
        "# ---------------- quick console preview by vehicle ----------------\n",
        "print(\"\\n=== Sample rows per common vehicle ===\")\n",
        "if \"Vehicle\" in out_df.columns:\n",
        "    top_vs = out_df[\"Vehicle\"].value_counts().head(3).index.tolist()\n",
        "    for v in top_vs:\n",
        "        sample = out_df[out_df[\"Vehicle\"] == v].head(3)\n",
        "        print(f\"\\nVehicle: {v} (showing up to 3 rows)\")\n",
        "        cols_show = [c for c in [\"Date\",\"Time\",\"Datetime\",\"segment_id\",\"Place\",\n",
        "                                 \"SPI_true\",\"SPI_pred\",\"thr_used\",\"is_high_true\",\"is_high_pred\"] if c in sample.columns]\n",
        "        print(sample[cols_show].to_string(index=False))\n",
        "else:\n",
        "    print(\"Vehicle column not found; skipped vehicle-wise samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4uxX5q6H7lS",
        "outputId": "11e24fbc-210d-4e4a-c1c0-1a22436c5e73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Loaded model: /content/models/xgb_vehicle_specific_risk.pkl\n",
            "[OK] Loaded data: /content/final_dataset.csv | shape=(315, 24)\n",
            "[OK] Selected last 63 rows as test set. Train=252 Test=63\n",
            "\n",
            "=== Regression metrics on last 63 ===\n",
            "R^2   : 0.652261\n",
            "MAE   : 0.009836\n",
            "RMSE  : 0.014979\n",
            "[OK] Loaded vehicle thresholds from /content/outputs/vehicle_thresholds.csv\n",
            "\n",
            "=== Alert metrics (per-vehicle thresholds) on last 63 ===\n",
            "Accuracy : 0.761905\n",
            "Precision: 0.900000\n",
            "Recall   : 0.765957\n",
            "F1-score : 0.827586\n",
            "\n",
            "[OK] Per-vehicle metrics saved to: /content/outputs/classification_metrics_per_vehicle.csv\n",
            "                    Vehicle  n_test  threshold_used  accuracy  precision   recall       f1\n",
            "              Three Wheeler      14        0.350718  0.857143        1.0 0.846154 0.916667\n",
            "                Motor Cycle       8        0.398337  1.000000        1.0 1.000000 1.000000\n",
            "                        Car       7        0.350718  0.428571        1.0 0.428571 0.600000\n",
            "                      Lorry       6        0.350718  0.666667        1.0 0.600000 0.750000\n",
            "                        Van       6        0.398337  1.000000        1.0 1.000000 1.000000\n",
            "                        Bus       5        0.398337  1.000000        1.0 1.000000 1.000000\n",
            "        Bus / Three Wheeler       4        0.350718  0.000000        0.0 0.000000 0.000000\n",
            "      Lorry / Three Wheeler       4        0.350718  1.000000        1.0 1.000000 1.000000\n",
            "          Bus / Motor Cycle       2        0.350718  0.500000        1.0 0.500000 0.666667\n",
            "                  Bus / Van       2        0.350718  1.000000        1.0 1.000000 1.000000\n",
            "        Car / Three Wheeler       1        0.350718  1.000000        1.0 1.000000 1.000000\n",
            "               Three Wheeer       1        0.350718  0.000000        0.0 0.000000 0.000000\n",
            "                Three Wheel       1        0.350718  1.000000        1.0 1.000000 1.000000\n",
            "      Three Wheeler / Lorry       1        0.350718  1.000000        1.0 1.000000 1.000000\n",
            "Three Wheeler / Motor Cycle       1        0.350718  0.000000        0.0 0.000000 0.000000\n",
            "\n",
            "[OK] Saved predictions (with thresholds & flags) to:\n",
            "  /content/outputs/predictions_from_pkl.csv\n",
            "[OK] Overall alert metrics saved to: classification_metrics_overall.json\n",
            "\n",
            "=== Sample rows per common vehicle ===\n",
            "\n",
            "Vehicle: Three Wheeler (showing up to 3 rows)\n",
            "      Date     Time            Datetime   segment_id                         Place  SPI_true  SPI_pred  thr_used  is_high_true  is_high_pred\n",
            "2023.12.18 13:25:00 2023-12-18 13:25:00 6.992_80.489          Ginigathena Hospital  0.398337  0.403113  0.350718             1             1\n",
            "2023.12.27 16:50:00 2023-12-27 16:50:00 6.995_80.492         Ginigathena Bus Stand  0.398337  0.413941  0.350718             1             1\n",
            "2024.02.04 15:00:00 2024-02-04 15:00:00 6.989_80.456 Millagahamula Dinuwara Stores  0.350718  0.350052  0.350718             1             0\n",
            "\n",
            "Vehicle: Motor Cycle (showing up to 3 rows)\n",
            "      Date     Time            Datetime   segment_id                                       Place  SPI_true  SPI_pred  thr_used  is_high_true  is_high_pred\n",
            "2023.12.06 14:35:00 2023-12-06 14:35:00 7.019_80.494                           Ambagamuwa Temple  0.350718  0.352798  0.398337             0             0\n",
            "2024.04.08 12:00:00 2024-04-08 12:00:00 6.995_80.492                       Ginigathena Bus Stand  0.334776  0.362172  0.398337             0             0\n",
            "2024.04.19 10:10:00 2024-04-19 10:10:00 6.989_80.494 Pedestrian Cross, Peoples Bank, Ginigathena  0.398337  0.416650  0.398337             1             1\n",
            "\n",
            "Vehicle: Car (showing up to 3 rows)\n",
            "      Date     Time            Datetime   segment_id                      Place  SPI_true  SPI_pred  thr_used  is_high_true  is_high_pred\n",
            "2023.12.20 09:25:00 2023-12-20 09:25:00  6.994_80.45          Pitawala Junction  0.350718  0.351585  0.350718             1             1\n",
            "2024.10.11 08:00:00 2024-10-11 08:00:00 6.998_80.477         Rampadeniya Temple  0.350718  0.355872  0.350718             1             1\n",
            "2024.10.18 03:00:00 2024-10-18 03:00:00   6.99_80.46 Millagahamula 46/8 Culvert  0.350718  0.345610  0.350718             1             0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wjQiUe7IYTtB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}